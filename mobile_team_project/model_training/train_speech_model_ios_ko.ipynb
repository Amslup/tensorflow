{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599969136337",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iOS 어플리케이션상 구동을 위한 커스텀 인식 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금부터 살펴볼 코드는 간단한 음성 인식을 위한 20kb 정도의 iOS 어플리케이션용 TensorFlow Lite 모델을 훈련하는 방법을 살펴볼 것입니다. 이 모델은 micro_speech 예제 애플리케이션에서 사용된 것과 같습니다.\n",
    "\n",
    "이 모델은 Google Colaboratory 환경에서도 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "[Run in Google Colab](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model_ios_ko.ipynb)\t[View source on GitHub](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model_ios_ko.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북의 코드는 Python 스크립트를 실행하여 훈련을 진행한 뒤, 모델을 고정시킨다. 그리고 Tensorflow Lite 모델 변환기를 통해 마이크로컨트롤러용 모델을 생성한다. \n",
    "\n",
    "**GPU 가속을 사용하면 훈련 속도가 훨씬 빨라진다.** 훈련을 하기에 앞서, 런타임 유형을 확인해보자. **Runtime -> Change runtime type**을 클릭하면 런타임을 **GPU** 환경으로 바꿀 수 있다. 훈련에는 GPU환경을 기준으로 18,000  이터레이션의 경우 1시간반에서 2시간 정도 걸린다.\n",
    "\n",
    "## 훈련 설정하기\n",
    "\n",
    "다음 `os.environ` 라인에서 학습할 단어, 훈련단계 및 학습률을 설정할 수 있다. 기본값은 micro_speech 예제에서 사용 된 것과 동일하다. 셀을 실행하여 설정을 진행하자:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의존성/종속성 설치\n",
    "\n",
    "최신버전 텐서플로우와 예제코드가 작성될 당시 사용했던 텐서플로우의 버전이 다르다\n",
    "버전 잘 맞춰야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: tensorflow==1.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.15.0)\nRequirement already satisfied: keras-applications>=1.0.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\nRequirement already satisfied: six>=1.10.0 in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (1.12.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\nRequirement already satisfied: tensorflow-estimator==1.15.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\nRequirement already satisfied: google-pasta>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\nRequirement already satisfied: gast==0.2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.18.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\nRequirement already satisfied: protobuf>=3.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (3.12.4)\nRequirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\nRequirement already satisfied: astor>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\nRequirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\nRequirement already satisfied: wheel>=0.26 in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (0.33.4)\nRequirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.9.0)\nRequirement already satisfied: wrapt>=1.11.1 in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (1.11.2)\nRequirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.31.0)\nRequirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\nRequirement already satisfied: setuptools in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (41.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.15.5)\n\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\nYou should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "# !git clone https://github.com/yunho0130/tensorflow-lite.git \n",
    "!git clone https://github.com/sanghunkang/tensorflow-lite.git\n",
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab에서 실행중이라면, 왼쪽의 디렉토리에 tensorflow-lite가 추가된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커스텀 데이터 생성 및 업로드\n",
    "\n",
    "훈련에 앞서서 우리가 훈련에 사용하고자 하는 데이터를 생성해야 한다. speech_commands 예제에서는 1초길이의 16bit wav파일을 훈련, 테스트 및 추론에 사용하고 있다. 각 예제\n",
    "\n",
    "ffmpeg 도구를 사용하면 변환작업을 빠르고 편리하게 수행할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!for i in *.wav; do ffmpeg -y -i \"$i\" -acodec pcm_s16le -ac 1 -ar 16000 \"tmp/${i%.*}.wav\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환한 파일은 클래스 별로 각각 하나씩 디렉토리로 만들어서 저장한 후, 이 디렉토리들을 다시 하나의 디렉토리 안에 모아놓도록 하자. 우리는 이제 이 데이터 루트 디렉토리 경로로 지정하여 우리의 목적에 맞는 모델을 훈련할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "이제까지는 딥러닝에서 모두가 손 대기를 꺼려하는 작업이었다. 지금부터는 딥러닝에서 재미에 해당하는 부분이다. 우리는 이제 speech_commands 예제에서 사용하는 모델의 아키텍쳐를 그대로 가져와서 그것을 우리만의 데이터셋에 적용할 것이다. 다음의 커맨드를 실행하면 모델이 훈련될 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/Library/Frameworks/Python.framework/Versions/3.7/Resources/Python.app/Contents/MacOS/Python: can't open file 'tensorflow-lite/tensorflow/examples/speech_commands/train.py': [Errno 2] No such file or directory\n"
    }
   ],
   "source": [
    "!python3 tensorflow-lite/tensorflow/examples/speech_commands/train.py \\\n",
    "    --model_architecture=conv \\\n",
    "    --how_many_training_steps=10000,100  \\\n",
    "    --train_dir=./retrain_logs \\\n",
    "    --data_dir=data\n",
    "    --wanted_words=bulyiya,suzy \\ \n",
    "    # --start_checkpoint=./retrain_logs/conv.ckpt-1100 # 만약 훈련을 중단한 모델을 중간부터 다시 훈련시키고자 한다면, 특정 체크포인트를 지정해서 그곳에서부터 다시 훈련을 시작할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "런타임이 모델을 훈련시키는 동안, 우리가 방금 실행시킨 커맨드가 어떤 작업을 하는지를 확인해보자.\n",
    "\n",
    "- ```python3 tensorflow-lite/tensorflow/examples/speech_commands/train.py```\n",
    "- ```--model_architecture=conv```\n",
    "- ```--how_many_training_steps=10000,100```\n",
    "- ```--train_dir=./retrain_logs```\n",
    "- ```--data_dir=data``` 앞에서 16bit .wav파일로 변환한 파일을 담고있는 폴더들의 상위폴더. 이 폴더는 bulyiya와 suzy 디렉토리를 하위 디렉토리로 가지고 있어야 한다.\n",
    "- ```--wanted_words=bulyiya,suzy```\n",
    "- ```--start_checkpoint=./retrain_logs/conv.ckpt-1100``` 만약 훈련을 중단한 모델을 중간부터 다시 훈련시키고자 한다면, 특정 체크포인트를 지정해서 그곳에서부터 다시 훈련을 시작할 수 있다.\n",
    "\n",
    "이 외에도 소스코드를 직접 수정하지 않고도 여러가지 조건을 변경하여 모델을 훈련시킬 수 있다. 이에 관한 상세한 내용은 ```tensorflow-lite/tensorflow/examples/speech_commands/train.py```의 FLAGS 입력 부분에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 고정\n",
    "\n",
    "주: 2.0 version부터는 saved_model이라고 하는 api를 지원한다. 기존의 pb로 만들어진 API보다 한단계 더 추상화가 된 API이다. 불행히도, pb모델과 saved_model간의 변환은 자유롭지 못하다. 그러나 다행인 것은 pb모델을 tflite모델로 변환하는 방법과 saved_model을 tflite모델로 변환하는 방법에는 크게 차이가 없다는 것이다. 따라서 본 예제의 내용흐름을 잘 따라올 수 있으면 2.x version의 텐서플로우에서도 큰 어려움 없이 tflite모델을 생성하는 파이프라인을 구성할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 tensorflow-lite/tensorflow/examples/speech_commands/freeze.py \\\n",
    "    --model_architecture=conv \\\n",
    "    --output_file=./content/retrain_logs/conv.pb \\\n",
    "    --start_checkpoint=./content/retrain_logs/conv.ckpt-1000 \\\n",
    "    --wanted_words=bulyiya,suzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 탐색\n",
    "\n",
    "파이썬 메모리상이 아닌 일반적인(?) 명칭으로서 우리가 훈련한 모델의 각 부분이 어떻게 되는지를 알아볼 필요가 있다. 그래프를 탐색해서 알아낸 내부구조에 특정 부분을 우리는 tflite모델을 만들면서 input과 output으로 모델 외부와 상호작용할 수 있는 채널로 개방할 것이다. 이 형식은 이후 iOS앱 개발에서 반드시 지켜줘야 하는 형식이므로 우리는 그 형식을 정확하게 iOS개발자에게 전달하기 위해서 모델의 그래프구조가 freezing 상태에서 어떻게 생겼는지 확인할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .tflite 파일 생성\n",
    "\n",
    "interpreter를 컴파일 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "^C\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/bin/toco\", line 5, in <module>\n    from tensorflow.lite.python.tflite_convert import main\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/__init__.py\", line 99, in <module>\n    from tensorflow_core import *\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/__init__.py\", line 28, in <module>\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n  File \"<frozen importlib._bootstrap>\", line 1019, in _handle_fromlist\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n    module = self._load()\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n    module = _importlib.import_module(self.__name__)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/__init__.py\", line 63, in <module>\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/framework_lib.py\", line 52, in <module>\n    from tensorflow.python.framework.importer import import_graph_def\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py\", line 28, in <module>\n    from tensorflow.python.framework import function\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/function.py\", line 37, in <module>\n    from tensorflow.python.ops import resource_variable_ops\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 40, in <module>\n    from tensorflow.python.ops import math_ops\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\", line 88, in <module>\n    from tensorflow.python.ops import gen_nn_ops\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\", line 3721, in <module>\n    FractionalMaxPool = _doc_controls.do_not_generate_docs(_kwarg_only(FractionalMaxPool))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/util/tf_export.py\", line 385, in kwarg_only\n    f_argspec = tf_inspect.getargspec(f)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/util/tf_inspect.py\", line 130, in getargspec\n    return _getargspec(target)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/util/tf_inspect.py\", line 71, in _getargspec\n    fullargspecs = getfullargspec(target)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/util/tf_inspect.py\", line 257, in getfullargspec\n    return _getfullargspec(target)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1126, in getfullargspec\n    sigcls=Signature)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 2284, in _signature_from_callable\n    return _signature_from_function(sigcls, obj)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 2154, in _signature_from_function\n    kind=_POSITIONAL_OR_KEYWORD))\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 2469, in __init__\n    self._kind = _ParameterKind(kind)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/enum.py\", line 310, in __call__\n    return cls.__new__(cls, value)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/enum.py\", line 526, in __new__\n    def __new__(cls, value):\nKeyboardInterrupt\n"
    }
   ],
   "source": [
    "!toco --graph_def_file=./content/retrain_logs/conv.pb --output_file=./content/retrain_logs/conv.tflite --input_arrays=decoded_sample_data,decoded_sample_data:1 --output_arrays=labels_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference\n",
    "\n",
    "wav로 들어올 지 bufferStream으로 들어올지를 결정하는 것은 iOS 개발자의 결정사항이지만, 일단 input레이어에 약속된대로 데이터를 꽂고 나면 output레이어에서 모델의 추론 결과를 약속한대로 뱉어내야 한다. 그것이 잘 되는지를 Python 코드 상에서 Interpreter와 예시 웨이브파일을 사용하여 확인할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"content/retrain_logs/conv.tflite\")\n",
    "print(interpreter.get_input_details())\n",
    "for tensor_detail in interpreter.get_tensor_details():\n",
    "    print(tensor_detail)\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "print(interpreter.get_input_details()[0][\"index\"])\n",
    "print(interpreter.get_input_details()[1][\"index\"])\n",
    "print(interpreter.get_output_details()[0][\"index\"])\n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "samplerate, data = wavfile.read(\"data/bulyiya/불이야_강상훈_1.wav\")\n",
    "print(samplerate, data.shape, data.dtype, data)\n",
    "input_data = np.array(data[:16000]/32767.0, dtype=np.float32).reshape((16000, 1))\n",
    "print(input_data.shape)\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n",
    "interpreter.set_tensor(interpreter.get_input_details()[1]['index'], np.int32(16000))\n",
    "\n",
    "interpreter.invoke()\n",
    "print(\"inference\", interpreter.get_tensor(interpreter.get_output_details()[0]['index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 진행되었으면 이제 iOS에 배포할 모델의 준비는 끝났다. 모델이 준비되었다는 기쁜 소식을 iOS개발자에게 전달하고, 우리는 퇴근할 준비를 하도록 하자!"
   ]
  }
 ]
}