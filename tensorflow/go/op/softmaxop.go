// Copyright 2017 The TensorFlow Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// DO NOT EDIT
// This file was machine generated by github.com/ctava/tensorflow/tensorflow/go/genop/wrap
//
// WARNING: This generation of wrapper function for TensorFlow ops is in an
// experimental state. The generated API can change without notice.

package op

import tf "github.com/tensorflow/tensorflow/tensorflow/go"

// Computes softmax cross entropy cost and gradients to backpropagate.
//
// Inputs are the logits, not probabilities.
//
// Arguments:
//	features: batch_size x num_classes matrix
//	labels: batch_size x num_classes matrix
// The caller must ensure that each batch of labels represents a valid
// probability distribution.
//
// Returns Per example loss (batch_size vector).backpropagated gradients (batch_size x num_classes matrix).
func SoftmaxCrossEntropyWithLogits(scope *Scope, features tf.Output, labels tf.Output)(loss tf.Output, backprop tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SoftmaxCrossEntropyWithLogits",
		Input: []tf.Input{
			features, labels, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Computes log softmax activations.
//
// For each batch `i` and class `j` we have
// 
//     logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))
//
// Arguments:
//	logits: 2-D with shape `[batch_size, num_classes]`.
//
// Returns Same shape as `logits`.
func LogSoftmax(scope *Scope, logits tf.Output)(logsoftmax tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "LogSoftmax",
		Input: []tf.Input{
			logits, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes softmax activations.
//
// For each batch `i` and class `j` we have
// 
//     softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))
//
// Arguments:
//	logits: 2-D with shape `[batch_size, num_classes]`.
//
// Returns Same shape as `logits`.
func Softmax(scope *Scope, logits tf.Output)(softmax tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Softmax",
		Input: []tf.Input{
			logits, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Applies softmax to a batched N-D `SparseTensor`.
//
// The inputs represent an N-D SparseTensor  with logical shape `[..., B, C]`
// (where `N >= 2`), and with indices sorted in the canonical lexicographic order.
// 
// This op is equivalent to applying the normal `tf.nn.softmax()` to each innermost
// logical submatrix with shape `[B, C]`, but with the catch that *the implicitly
// zero elements do not participate*.  Specifically, the algorithm is equivalent
// to the following:
// 
//   (1) Applies `tf.nn.softmax()` to a densified view of each innermost submatrix
//       with shape `[B, C]`, along the size-C dimension;
//   (2) Masks out the original implicitly-zero locations;
//   (3) Renormalizes the remaining elements.
// 
// Hence, the `SparseTensor` result has exactly the same non-zero indices and
// shape.
//
// Arguments:
//	sp_indices: 2-D.  `NNZ x R` matrix with the indices of non-empty values in a
// SparseTensor, in canonical ordering.
//	sp_values: 1-D.  `NNZ` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//
// Returns 1-D.  The `NNZ` values for the result `SparseTensor`.
func SparseSoftmax(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSoftmax",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes softmax cross entropy cost and gradients to backpropagate.
//
// Unlike `SoftmaxCrossEntropyWithLogits`, this operation does not accept
// a matrix of label probabilities, but rather a single label per row
// of features.  This label is considered to have probability 1.0 for the
// given row.
// 
// Inputs are the logits, not probabilities.
//
// Arguments:
//	features: batch_size x num_classes matrix
//	labels: batch_size vector with values in [0, num_classes).
// This is the label for the given minibatch entry.
//
// Returns Per example loss (batch_size vector).backpropagated gradients (batch_size x num_classes matrix).
func SparseSoftmaxCrossEntropyWithLogits(scope *Scope, features tf.Output, labels tf.Output)(loss tf.Output, backprop tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSoftmaxCrossEntropyWithLogits",
		Input: []tf.Input{
			features, labels, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Computes softmax cross entropy cost and gradients to backpropagate.
//
// Inputs are the logits, not probabilities.
//
// Arguments:
//	features: batch_size x num_classes matrix
//	labels: batch_size x num_classes matrix
// The caller must ensure that each batch of labels represents a valid
// probability distribution.
//
// Returns Per example loss (batch_size vector).backpropagated gradients (batch_size x num_classes matrix).
func SoftmaxCrossEntropyWithLogits(scope *Scope, features tf.Output, labels tf.Output)(loss tf.Output, backprop tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SoftmaxCrossEntropyWithLogits",
		Input: []tf.Input{
			features, labels, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Computes log softmax activations.
//
// For each batch `i` and class `j` we have
// 
//     logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))
//
// Arguments:
//	logits: 2-D with shape `[batch_size, num_classes]`.
//
// Returns Same shape as `logits`.
func LogSoftmax(scope *Scope, logits tf.Output)(logsoftmax tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "LogSoftmax",
		Input: []tf.Input{
			logits, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes softmax activations.
//
// For each batch `i` and class `j` we have
// 
//     softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))
//
// Arguments:
//	logits: 2-D with shape `[batch_size, num_classes]`.
//
// Returns Same shape as `logits`.
func Softmax(scope *Scope, logits tf.Output)(softmax tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Softmax",
		Input: []tf.Input{
			logits, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Applies softmax to a batched N-D `SparseTensor`.
//
// The inputs represent an N-D SparseTensor  with logical shape `[..., B, C]`
// (where `N >= 2`), and with indices sorted in the canonical lexicographic order.
// 
// This op is equivalent to applying the normal `tf.nn.softmax()` to each innermost
// logical submatrix with shape `[B, C]`, but with the catch that *the implicitly
// zero elements do not participate*.  Specifically, the algorithm is equivalent
// to the following:
// 
//   (1) Applies `tf.nn.softmax()` to a densified view of each innermost submatrix
//       with shape `[B, C]`, along the size-C dimension;
//   (2) Masks out the original implicitly-zero locations;
//   (3) Renormalizes the remaining elements.
// 
// Hence, the `SparseTensor` result has exactly the same non-zero indices and
// shape.
//
// Arguments:
//	sp_indices: 2-D.  `NNZ x R` matrix with the indices of non-empty values in a
// SparseTensor, in canonical ordering.
//	sp_values: 1-D.  `NNZ` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//
// Returns 1-D.  The `NNZ` values for the result `SparseTensor`.
func SparseSoftmax(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSoftmax",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes softmax cross entropy cost and gradients to backpropagate.
//
// Unlike `SoftmaxCrossEntropyWithLogits`, this operation does not accept
// a matrix of label probabilities, but rather a single label per row
// of features.  This label is considered to have probability 1.0 for the
// given row.
// 
// Inputs are the logits, not probabilities.
//
// Arguments:
//	features: batch_size x num_classes matrix
//	labels: batch_size vector with values in [0, num_classes).
// This is the label for the given minibatch entry.
//
// Returns Per example loss (batch_size vector).backpropagated gradients (batch_size x num_classes matrix).
func SparseSoftmaxCrossEntropyWithLogits(scope *Scope, features tf.Output, labels tf.Output)(loss tf.Output, backprop tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSoftmaxCrossEntropyWithLogits",
		Input: []tf.Input{
			features, labels, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}
