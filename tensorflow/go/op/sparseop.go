// Copyright 2017 The TensorFlow Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// DO NOT EDIT
// This file was machine generated by github.com/ctava/tensorflow/tensorflow/go/genop/wrap
//
// WARNING: This generation of wrapper function for TensorFlow ops is in an
// experimental state. The generated API can change without notice.

package op

import tf "github.com/tensorflow/tensorflow/tensorflow/go"

// Computes the mean along sparse segments of a tensor.
//
// Read @{$math_ops#segmentation$the section on segmentation} for an explanation of
// segments.
// 
// Like `SegmentMean`, but `segment_ids` can have rank less than `data`'s first
// dimension, selecting a subset of dimension 0, specified by `indices`.
//
// Arguments:
//	
//	indices: A 1-D tensor. Has same rank as `segment_ids`.
//	segment_ids: A 1-D tensor. Values should be sorted and can be repeated.
//
// Returns Has same shape as data, except for dimension 0 which
// has size `k`, the number of segments.
func SparseSegmentMean(scope *Scope, data tf.Output, indices tf.Output, segment_ids tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentMean",
		Input: []tf.Input{
			data, indices, segment_ids, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes the sum along sparse segments of a tensor.
//
// Read @{$math_ops#segmentation$the section on segmentation} for an explanation of
// segments.
// 
// Like `SegmentSum`, but `segment_ids` can have rank less than `data`'s first
// dimension, selecting a subset of dimension 0, specified by `indices`.
// 
// For example:
// 
// ```prettyprint
// c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])
// 
// # Select two rows, one segment.
// tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))
//   ==> [[0 0 0 0]]
// 
// # Select two rows, two segment.
// tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))
//   ==> [[ 1  2  3  4]
//        [-1 -2 -3 -4]]
// 
// # Select all rows, two segments.
// tf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))
//   ==> [[0 0 0 0]
//        [5 6 7 8]]
// 
// # Which is equivalent to:
// tf.segment_sum(c, tf.constant([0, 0, 1]))
// ```
//
// Arguments:
//	
//	indices: A 1-D tensor. Has same rank as `segment_ids`.
//	segment_ids: A 1-D tensor. Values should be sorted and can be repeated.
//
// Returns Has same shape as data, except for dimension 0 which
// has size `k`, the number of segments.
func SparseSegmentSum(scope *Scope, data tf.Output, indices tf.Output, segment_ids tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentSum",
		Input: []tf.Input{
			data, indices, segment_ids, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseMatMulAttr is an optional argument to SparseMatMul.
type SparseMatMulAttr func(optionalAttr)


// SparseMatMulTransposeA sets the optional transpose_a attribute to value.
// If not specified, defaults to false 
func SparseMatMulTransposeA(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["transpose_a"] = value
	}
}

// SparseMatMulTransposeB sets the optional transpose_b attribute to value.
// If not specified, defaults to false 
func SparseMatMulTransposeB(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["transpose_b"] = value
	}
}

// SparseMatMulAIsSparse sets the optional a_is_sparse attribute to value.
// If not specified, defaults to false 
func SparseMatMulAIsSparse(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["a_is_sparse"] = value
	}
}

// SparseMatMulBIsSparse sets the optional b_is_sparse attribute to value.
// If not specified, defaults to false 
func SparseMatMulBIsSparse(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["b_is_sparse"] = value
	}
}

// Multiply matrix "a" by matrix "b".
//
// The inputs must be two-dimensional matrices and the inner dimension of "a" must
// match the outer dimension of "b". This op is optimized for the case where at
// least one of "a" or "b" is sparse. The breakeven for using this versus a dense
// matrix multiply on one platform was 30% zero values in the sparse matrix.
func SparseMatMul(scope *Scope, a tf.Output, b tf.Output, optional ...SparseMatMulAttr)(product tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseMatMul",
		Input: []tf.Input{
			a, b, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes gradients for SparseSegmentSqrtN.
//
// Returns tensor "output" with same shape as grad, except for dimension 0 whose
// value is output_dim0.
//
// Arguments:
//	grad: gradient propagated to the SparseSegmentSqrtN op.
//	indices: indices passed to the corresponding SparseSegmentSqrtN op.
//	segment_ids: segment_ids passed to the corresponding SparseSegmentSqrtN op.
//	output_dim0: dimension 0 of "data" passed to SparseSegmentSqrtN op.
func SparseSegmentSqrtNGrad(scope *Scope, grad tf.Output, indices tf.Output, segment_ids tf.Output, output_dim0 tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentSqrtNGrad",
		Input: []tf.Input{
			grad, indices, segment_ids, output_dim0, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes the sum along sparse segments of a tensor divided by the sqrt of N.
//
// N is the size of the segment being reduced.
// 
// Read @{$math_ops#segmentation$the section on segmentation} for an explanation of
// segments.
//
// Arguments:
//	
//	indices: A 1-D tensor. Has same rank as `segment_ids`.
//	segment_ids: A 1-D tensor. Values should be sorted and can be repeated.
//
// Returns Has same shape as data, except for dimension 0 which
// has size `k`, the number of segments.
func SparseSegmentSqrtN(scope *Scope, data tf.Output, indices tf.Output, segment_ids tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentSqrtN",
		Input: []tf.Input{
			data, indices, segment_ids, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseReduceSumSparseAttr is an optional argument to SparseReduceSumSparse.
type SparseReduceSumSparseAttr func(optionalAttr)


// SparseReduceSumSparseKeepDims sets the optional keep_dims attribute to value.
//
// value: If true, retain reduced dimensions with length 1.
// If not specified, defaults to false 
func SparseReduceSumSparseKeepDims(value bool) SparseReduceSumSparseAttr {
	return func(m optionalAttr) {
		m["keep_dims"] = value
	}
}

// Computes the sum of elements across dimensions of a SparseTensor.
//
// This Op takes a SparseTensor and is the sparse counterpart to
// `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a
// SparseTensor.
// 
// Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
// `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
// `reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
// with length 1.
// 
// If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
// with a single element is returned.  Additionally, the axes can be negative,
// which are interpreted according to the indexing rules in Python.
//
// Arguments:
//	input_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	input_values: 1-D.  `N` non-empty values corresponding to `input_indices`.
//	input_shape: 1-D.  Shape of the input SparseTensor.
//	reduction_axes: 1-D.  Length-`K` vector containing the reduction axes.
func SparseReduceSumSparse(scope *Scope, input_indices tf.Output, input_values tf.Output, input_shape tf.Output, reduction_axes tf.Output, optional ...SparseReduceSumSparseAttr)(output_indices tf.Output, output_values tf.Output, output_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseReduceSumSparse",
		Input: []tf.Input{
			input_indices, input_values, input_shape, reduction_axes, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// ResourceSparseApplyRMSPropAttr is an optional argument to ResourceSparseApplyRMSProp.
type ResourceSparseApplyRMSPropAttr func(optionalAttr)


// ResourceSparseApplyRMSPropUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var, ms, and mom tensors is protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyRMSPropUseLocking(value bool) ResourceSparseApplyRMSPropAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update '*var' according to the RMSProp algorithm.
//
// Note that in dense implementation of this algorithm, ms and mom will
// update even if the grad is zero, but in this sparse implementation, ms
// and mom will not update in iterations during which the grad is zero.
// 
// mean_square = decay * mean_square + (1-decay) * gradient ** 2
// Delta = learning_rate * gradient / sqrt(mean_square + epsilon)
// 
// ms <- rho * ms_{t-1} + (1-rho) * grad * grad
// mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
// var <- var - mom
//
// Arguments:
//	var_: Should be from a Variable().
//	ms: Should be from a Variable().
//	mom: Should be from a Variable().
//	lr: Scaling factor. Must be a scalar.
//	rho: Decay rate. Must be a scalar.
//	
//	epsilon: Ridge term. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var, ms and mom.
//
// Returns the created operation.
func ResourceSparseApplyRMSProp(scope *Scope, var_ tf.Output, ms tf.Output, mom tf.Output, lr tf.Output, rho tf.Output, momentum tf.Output, epsilon tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyRMSPropAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyRMSProp",
		Input: []tf.Input{
			var_, ms, mom, lr, rho, momentum, epsilon, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// Adds two `SparseTensor` objects to produce another `SparseTensor`.
//
// The input `SparseTensor` objects' indices are assumed ordered in standard
// lexicographic order.  If this is not the case, before this step run
// `SparseReorder` to restore index ordering.
// 
// By default, if two values sum to zero at some index, the output `SparseTensor`
// would still include that particular location in its index, storing a zero in the
// corresponding value slot.  To override this, callers can specify `thresh`,
// indicating that if the sum has a magnitude strictly smaller than `thresh`, its
// corresponding value and index would then not be included.  In particular,
// `thresh == 0` (default) means everything is kept and actual thresholding happens
// only for a positive value.
// 
// In the following shapes, `nnz` is the count after taking `thresh` into account.
//
// Arguments:
//	a_indices: 2-D.  The `indices` of the first `SparseTensor`, size `[nnz, ndims]` Matrix.
//	a_values: 1-D.  The `values` of the first `SparseTensor`, size `[nnz]` Vector.
//	a_shape: 1-D.  The `shape` of the first `SparseTensor`, size `[ndims]` Vector.
//	b_indices: 2-D.  The `indices` of the second `SparseTensor`, size `[nnz, ndims]` Matrix.
//	b_values: 1-D.  The `values` of the second `SparseTensor`, size `[nnz]` Vector.
//	b_shape: 1-D.  The `shape` of the second `SparseTensor`, size `[ndims]` Vector.
//	thresh: 0-D.  The magnitude threshold that determines if an output value/index
// pair takes space.
func SparseAdd(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b_indices tf.Output, b_values tf.Output, b_shape tf.Output, thresh tf.Output)(sum_indices tf.Output, sum_values tf.Output, sum_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseAdd",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b_indices, b_values, b_shape, thresh, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// ResourceSparseApplyMomentumAttr is an optional argument to ResourceSparseApplyMomentum.
type ResourceSparseApplyMomentumAttr func(optionalAttr)


// ResourceSparseApplyMomentumUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var and accum tensors will be protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyMomentumUseLocking(value bool) ResourceSparseApplyMomentumAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// ResourceSparseApplyMomentumUseNesterov sets the optional use_nesterov attribute to value.
//
// value: If `True`, the tensor passed to compute grad will be
// var - lr * momentum * accum, so in the end, the var you get is actually
// var - lr * momentum * accum.
// If not specified, defaults to false 
func ResourceSparseApplyMomentumUseNesterov(value bool) ResourceSparseApplyMomentumAttr {
	return func(m optionalAttr) {
		m["use_nesterov"] = value
	}
}

// Update relevant entries in '*var' and '*accum' according to the momentum scheme.
//
// Set use_nesterov = True if you want to use Nesterov momentum.
// 
// That is for rows we have grad for, we update var and accum as follows:
// 
// accum = accum * momentum + grad
// var -= lr * accum
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//	momentum: Momentum. Must be a scalar.
//
// Returns the created operation.
func ResourceSparseApplyMomentum(scope *Scope, var_ tf.Output, accum tf.Output, lr tf.Output, grad tf.Output, indices tf.Output, momentum tf.Output, optional ...ResourceSparseApplyMomentumAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyMomentum",
		Input: []tf.Input{
			var_, accum, lr, grad, indices, momentum, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyProximalGradientDescentAttr is an optional argument to ResourceSparseApplyProximalGradientDescent.
type ResourceSparseApplyProximalGradientDescentAttr func(optionalAttr)


// ResourceSparseApplyProximalGradientDescentUseLocking sets the optional use_locking attribute to value.
//
// value: If True, the subtraction will be protected by a lock;
// otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyProximalGradientDescentUseLocking(value bool) ResourceSparseApplyProximalGradientDescentAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Sparse update '*var' as FOBOS algorithm with fixed learning rate.
//
// That is for rows we have grad for, we update var as follows:
// prox_v = var - alpha * grad
// var = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}
//
// Arguments:
//	var_: Should be from a Variable().
//	alpha: Scaling factor. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyProximalGradientDescent(scope *Scope, var_ tf.Output, alpha tf.Output, l1 tf.Output, l2 tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyProximalGradientDescentAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyProximalGradientDescent",
		Input: []tf.Input{
			var_, alpha, l1, l2, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyProximalAdagradAttr is an optional argument to ResourceSparseApplyProximalAdagrad.
type ResourceSparseApplyProximalAdagradAttr func(optionalAttr)


// ResourceSparseApplyProximalAdagradUseLocking sets the optional use_locking attribute to value.
//
// value: If True, updating of the var and accum tensors will be protected by
// a lock; otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyProximalAdagradUseLocking(value bool) ResourceSparseApplyProximalAdagradAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Sparse update entries in '*var' and '*accum' according to FOBOS algorithm.
//
// That is for rows we have grad for, we update var and accum as follows:
// accum += grad * grad
// prox_v = var
// prox_v -= lr * grad * (1 / sqrt(accum))
// var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyProximalAdagrad(scope *Scope, var_ tf.Output, accum tf.Output, lr tf.Output, l1 tf.Output, l2 tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyProximalAdagradAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyProximalAdagrad",
		Input: []tf.Input{
			var_, accum, lr, l1, l2, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyAdagradDAAttr is an optional argument to ResourceSparseApplyAdagradDA.
type ResourceSparseApplyAdagradDAAttr func(optionalAttr)


// ResourceSparseApplyAdagradDAUseLocking sets the optional use_locking attribute to value.
//
// value: If True, updating of the var and accum tensors will be protected by
// a lock; otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyAdagradDAUseLocking(value bool) ResourceSparseApplyAdagradDAAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update entries in '*var' and '*accum' according to the proximal adagrad scheme.
//
// Arguments:
//	var_: Should be from a Variable().
//	gradient_accumulator: Should be from a Variable().
//	gradient_squared_accumulator: Should be from a Variable().
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//	lr: Learning rate. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	global_step: Training step number. Must be a scalar.
//
// Returns the created operation.
func ResourceSparseApplyAdagradDA(scope *Scope, var_ tf.Output, gradient_accumulator tf.Output, gradient_squared_accumulator tf.Output, grad tf.Output, indices tf.Output, lr tf.Output, l1 tf.Output, l2 tf.Output, global_step tf.Output, optional ...ResourceSparseApplyAdagradDAAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyAdagradDA",
		Input: []tf.Input{
			var_, gradient_accumulator, gradient_squared_accumulator, grad, indices, lr, l1, l2, global_step, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyCenteredRMSPropAttr is an optional argument to ResourceSparseApplyCenteredRMSProp.
type ResourceSparseApplyCenteredRMSPropAttr func(optionalAttr)


// ResourceSparseApplyCenteredRMSPropUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var, mg, ms, and mom tensors is
// protected by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyCenteredRMSPropUseLocking(value bool) ResourceSparseApplyCenteredRMSPropAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update '*var' according to the centered RMSProp algorithm.
//
// The centered RMSProp algorithm uses an estimate of the centered second moment
// (i.e., the variance) for normalization, as opposed to regular RMSProp, which
// uses the (uncentered) second moment. This often helps with training, but is
// slightly more expensive in terms of computation and memory.
// 
// Note that in dense implementation of this algorithm, mg, ms, and mom will
// update even if the grad is zero, but in this sparse implementation, mg, ms,
// and mom will not update in iterations during which the grad is zero.
// 
// mean_square = decay * mean_square + (1-decay) * gradient ** 2
// mean_grad = decay * mean_grad + (1-decay) * gradient
// Delta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)
// 
// ms <- rho * ms_{t-1} + (1-rho) * grad * grad
// mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
// var <- var - mom
//
// Arguments:
//	var_: Should be from a Variable().
//	mg: Should be from a Variable().
//	ms: Should be from a Variable().
//	mom: Should be from a Variable().
//	lr: Scaling factor. Must be a scalar.
//	rho: Decay rate. Must be a scalar.
//	
//	epsilon: Ridge term. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var, ms and mom.
//
// Returns the created operation.
func ResourceSparseApplyCenteredRMSProp(scope *Scope, var_ tf.Output, mg tf.Output, ms tf.Output, mom tf.Output, lr tf.Output, rho tf.Output, momentum tf.Output, epsilon tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyCenteredRMSPropAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyCenteredRMSProp",
		Input: []tf.Input{
			var_, mg, ms, mom, lr, rho, momentum, epsilon, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyAdadeltaAttr is an optional argument to ResourceSparseApplyAdadelta.
type ResourceSparseApplyAdadeltaAttr func(optionalAttr)


// ResourceSparseApplyAdadeltaUseLocking sets the optional use_locking attribute to value.
//
// value: If True, updating of the var and accum tensors will be protected by
// a lock; otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyAdadeltaUseLocking(value bool) ResourceSparseApplyAdadeltaAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// var: Should be from a Variable().
//
// Arguments:
//	
//	accum: Should be from a Variable().
//	accum_update: : Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	rho: Decay factor. Must be a scalar.
//	epsilon: Constant factor. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyAdadelta(scope *Scope, var_ tf.Output, accum tf.Output, accum_update tf.Output, lr tf.Output, rho tf.Output, epsilon tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyAdadeltaAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyAdadelta",
		Input: []tf.Input{
			var_, accum, accum_update, lr, rho, epsilon, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// TakeManySparseFromTensorsMapAttr is an optional argument to TakeManySparseFromTensorsMap.
type TakeManySparseFromTensorsMapAttr func(optionalAttr)


// TakeManySparseFromTensorsMapContainer sets the optional container attribute to value.
//
// value: The container name for the `SparseTensorsMap` read by this op.
// If not specified, defaults to "" 
func TakeManySparseFromTensorsMapContainer(value string) TakeManySparseFromTensorsMapAttr {
	return func(m optionalAttr) {
		m["container"] = value
	}
}

// TakeManySparseFromTensorsMapSharedName sets the optional shared_name attribute to value.
//
// value: The shared name for the `SparseTensorsMap` read by this op.
// It should not be blank; rather the `shared_name` or unique Operation name
// of the Op that created the original `SparseTensorsMap` should be used.
// If not specified, defaults to "" 
func TakeManySparseFromTensorsMapSharedName(value string) TakeManySparseFromTensorsMapAttr {
	return func(m optionalAttr) {
		m["shared_name"] = value
	}
}

// Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.
//
// The input `sparse_handles` must be an `int64` matrix of shape `[N, 1]` where
// `N` is the minibatch size and the rows correspond to the output handles of
// `AddSparseToTensorsMap` or `AddManySparseToTensorsMap`.  The ranks of the
// original `SparseTensor` objects that went into the given input ops must all
// match.  When the final `SparseTensor` is created, it has rank one
// higher than the ranks of the incoming `SparseTensor` objects
// (they have been concatenated along a new row dimension on the left).
// 
// The output `SparseTensor` object's shape values for all dimensions but the
// first are the max across the input `SparseTensor` objects' shape values
// for the corresponding dimensions.  Its first shape value is `N`, the minibatch
// size.
// 
// The input `SparseTensor` objects' indices are assumed ordered in
// standard lexicographic order.  If this is not the case, after this
// step run `SparseReorder` to restore index ordering.
// 
// For example, if the handles represent an input, which is a `[2, 3]` matrix
// representing two original `SparseTensor` objects:
// 
// ```
//     index = [ 0]
//             [10]
//             [20]
//     values = [1, 2, 3]
//     shape = [50]
// ```
// 
// and
// 
// ```
//     index = [ 2]
//             [10]
//     values = [4, 5]
//     shape = [30]
// ```
// 
// then the final `SparseTensor` will be:
// 
// ```
//     index = [0  0]
//             [0 10]
//             [0 20]
//             [1  2]
//             [1 10]
//     values = [1, 2, 3, 4, 5]
//     shape = [2 50]
// ```
//
// Arguments:
//	sparse_handles: 1-D, The `N` serialized `SparseTensor` objects.
// Shape: `[N]`.
//	dtype: The `dtype` of the `SparseTensor` objects stored in the
// `SparseTensorsMap`.
//
// Returns 2-D.  The `indices` of the minibatch `SparseTensor`.1-D.  The `values` of the minibatch `SparseTensor`.1-D.  The `shape` of the minibatch `SparseTensor`.
func TakeManySparseFromTensorsMap(scope *Scope, sparse_handles tf.Output, dtype tf.DataType, optional ...TakeManySparseFromTensorsMapAttr)(sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"dtype": dtype,}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "TakeManySparseFromTensorsMap",
		Input: []tf.Input{
			sparse_handles, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// ResourceSparseApplyFtrlAttr is an optional argument to ResourceSparseApplyFtrl.
type ResourceSparseApplyFtrlAttr func(optionalAttr)


// ResourceSparseApplyFtrlUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var and accum tensors will be protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyFtrlUseLocking(value bool) ResourceSparseApplyFtrlAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update relevant entries in '*var' according to the Ftrl-proximal scheme.
//
// That is for rows we have grad for, we update var, accum and linear as follows:
// accum_new = accum + grad * grad
// linear += grad + (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
// quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
// var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
// accum = accum_new
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	linear: Should be from a Variable().
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//	lr: Scaling factor. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	lr_power: Scaling factor. Must be a scalar.
//
// Returns the created operation.
func ResourceSparseApplyFtrl(scope *Scope, var_ tf.Output, accum tf.Output, linear tf.Output, grad tf.Output, indices tf.Output, lr tf.Output, l1 tf.Output, l2 tf.Output, lr_power tf.Output, optional ...ResourceSparseApplyFtrlAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyFtrl",
		Input: []tf.Input{
			var_, accum, linear, grad, indices, lr, l1, l2, lr_power, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyAdagradAttr is an optional argument to ResourceSparseApplyAdagrad.
type ResourceSparseApplyAdagradAttr func(optionalAttr)


// ResourceSparseApplyAdagradUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var and accum tensors will be protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyAdagradUseLocking(value bool) ResourceSparseApplyAdagradAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update relevant entries in '*var' and '*accum' according to the adagrad scheme.
//
// That is for rows we have grad for, we update var and accum as follows:
// accum += grad * grad
// var -= lr * grad * (1 / sqrt(accum))
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyAdagrad(scope *Scope, var_ tf.Output, accum tf.Output, lr tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyAdagradAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyAdagrad",
		Input: []tf.Input{
			var_, accum, lr, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// Concatenates a list of `SparseTensor` along the specified dimension.
//
// Concatenation is with respect to the dense versions of these sparse tensors.
// It is assumed that each input is a `SparseTensor` whose elements are ordered
// along increasing dimension number.
// 
// All inputs' shapes must match, except for the concat dimension.  The
// `indices`, `values`, and `shapes` lists must have the same length.
// 
// The output shape is identical to the inputs', except along the concat
// dimension, where it is the sum of the inputs' sizes along that dimension.
// 
// The output elements will be resorted to preserve the sort order along
// increasing dimension number.
// 
// This op runs in `O(M log M)` time, where `M` is the total number of non-empty
// values across all inputs. This is due to the need for an internal sort in
// order to concatenate efficiently across an arbitrary dimension.
// 
// For example, if `concat_dim = 1` and the inputs are
// 
//     sp_inputs[0]: shape = [2, 3]
//     [0, 2]: "a"
//     [1, 0]: "b"
//     [1, 1]: "c"
// 
//     sp_inputs[1]: shape = [2, 4]
//     [0, 1]: "d"
//     [0, 2]: "e"
// 
// then the output will be
// 
//     shape = [2, 7]
//     [0, 2]: "a"
//     [0, 4]: "d"
//     [0, 5]: "e"
//     [1, 0]: "b"
//     [1, 1]: "c"
// 
// Graphically this is equivalent to doing
// 
//     [    a] concat [  d e  ] = [    a   d e  ]
//     [b c  ]        [       ]   [b c          ]
//
// Arguments:
//	indices: 2-D.  Indices of each input `SparseTensor`.
//	values: 1-D.  Non-empty values of each `SparseTensor`.
//	shapes: 1-D.  Shapes of each `SparseTensor`.
//	concat_dim: Dimension to concatenate along. Must be in range [-rank, rank),
// where rank is the number of dimensions in each input `SparseTensor`.
//
// Returns 2-D.  Indices of the concatenated `SparseTensor`.1-D.  Non-empty values of the concatenated `SparseTensor`.1-D.  Shape of the concatenated `SparseTensor`.
func SparseConcat(scope *Scope, indices []tf.Output, values []tf.Output, shapes []tf.Output, concat_dim int64)(output_indices tf.Output, output_values tf.Output, output_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"concat_dim": concat_dim,}
	opspec := tf.OpSpec{
		Type: "SparseConcat",
		Input: []tf.Input{
			tf.OutputList(indices), tf.OutputList(values), tf.OutputList(shapes), 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// Returns the element-wise min of two SparseTensors.
//
// Assumes the two SparseTensors have the same shape, i.e., no broadcasting.
//
// Arguments:
//	a_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, in the canonical lexicographic ordering.
//	a_values: 1-D.  `N` non-empty values corresponding to `a_indices`.
//	a_shape: 1-D.  Shape of the input SparseTensor.
//	b_indices: counterpart to `a_indices` for the other operand.
//	b_values: counterpart to `a_values` for the other operand; must be of the same dtype.
//	b_shape: counterpart to `a_shape` for the other operand; the two shapes must be equal.
//
// Returns 2-D.  The indices of the output SparseTensor.1-D.  The values of the output SparseTensor.
func SparseSparseMinimum(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b_indices tf.Output, b_values tf.Output, b_shape tf.Output)(output_indices tf.Output, output_values tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSparseMinimum",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b_indices, b_values, b_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Adds up a SparseTensor and a dense Tensor, using these special rules:
//
// (1) Broadcasts the dense side to have the same shape as the sparse side, if
//     eligible;
// (2) Then, only the dense values pointed to by the indices of the SparseTensor
//     participate in the cwise addition.
// 
// By these rules, the result is a logical SparseTensor with exactly the same
// indices and shape, but possibly with different non-zero values.  The output of
// this Op is the resultant non-zero values.
//
// Arguments:
//	sp_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	sp_values: 1-D.  `N` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//	dense: `R`-D.  The dense Tensor operand.
//
// Returns 1-D.  The `N` values that are operated on.
func SparseDenseCwiseAdd(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output, dense tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseDenseCwiseAdd",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, dense, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Component-wise multiplies a SparseTensor by a dense Tensor.
//
// The output locations corresponding to the implicitly zero elements in the sparse
// tensor will be zero (i.e., will not take up storage space), regardless of the
// contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).
// 
// *Limitation*: this Op only broadcasts the dense side to the sparse side, but not
// the other direction.
//
// Arguments:
//	sp_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	sp_values: 1-D.  `N` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//	dense: `R`-D.  The dense Tensor operand.
//
// Returns 1-D.  The `N` values that are operated on.
func SparseDenseCwiseMul(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output, dense tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseDenseCwiseMul",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, dense, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Adds up a `SparseTensor` and a dense `Tensor`, producing a dense `Tensor`.
//
// This Op does not require `a_indices` be sorted in standard lexicographic order.
//
// Arguments:
//	a_indices: 2-D.  The `indices` of the `SparseTensor`, with shape `[nnz, ndims]`.
//	a_values: 1-D.  The `values` of the `SparseTensor`, with shape `[nnz]`.
//	a_shape: 1-D.  The `shape` of the `SparseTensor`, with shape `[ndims]`.
//	b: `ndims`-D Tensor.  With shape `a_shape`.
func SparseTensorDenseAdd(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseTensorDenseAdd",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Reorders a SparseTensor into the canonical, row-major ordering.
//
// Note that by convention, all sparse ops preserve the canonical ordering along
// increasing dimension number. The only time ordering can be violated is during
// manual manipulation of the indices and values vectors to add entries.
// 
// Reordering does not affect the shape of the SparseTensor.
// 
// If the tensor has rank `R` and `N` non-empty values, `input_indices` has
// shape `[N, R]`, input_values has length `N`, and input_shape has length `R`.
//
// Arguments:
//	input_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	input_values: 1-D.  `N` non-empty values corresponding to `input_indices`.
//	input_shape: 1-D.  Shape of the input SparseTensor.
//
// Returns 2-D.  `N x R` matrix with the same indices as input_indices, but
// in canonical row-major ordering.1-D.  `N` non-empty values corresponding to `output_indices`.
func SparseReorder(scope *Scope, input_indices tf.Output, input_values tf.Output, input_shape tf.Output)(output_indices tf.Output, output_values tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseReorder",
		Input: []tf.Input{
			input_indices, input_values, input_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Split a `SparseTensor` into `num_split` tensors along one dimension.
//
// If the `shape[split_dim]` is not an integer multiple of `num_split`. Slices
// `[0 : shape[split_dim] % num_split]` gets one extra dimension.
// For example, if `split_dim = 1` and `num_split = 2` and the input is
// 
//     input_tensor = shape = [2, 7]
//     [    a   d e  ]
//     [b c          ]
// 
// Graphically the output tensors are:
// 
//     output_tensor[0] = shape = [2, 4]
//     [    a  ]
//     [b c    ]
// 
//     output_tensor[1] = shape = [2, 3]
//     [ d e  ]
//     [      ]
//
// Arguments:
//	split_dim: 0-D.  The dimension along which to split.  Must be in the range
// `[0, rank(shape))`.
//	indices: 2-D tensor represents the indices of the sparse tensor.
//	values: 1-D tensor represents the values of the sparse tensor.
//	shape: 1-D. tensor represents the shape of the sparse tensor.
// output indices: A list of 1-D tensors represents the indices of the output
// sparse tensors.
//	num_split: The number of ways to split.
//
// Returns A list of 1-D tensors represents the values of the output sparse
// tensors.A list of 1-D tensors represents the shape of the output sparse
// tensors.
func SparseSplit(scope *Scope, split_dim tf.Output, indices tf.Output, values tf.Output, shape tf.Output, num_split int64)(output_indices []tf.Output, output_values []tf.Output, output_shape []tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"num_split": num_split,}
	opspec := tf.OpSpec{
		Type: "SparseSplit",
		Input: []tf.Input{
			split_dim, indices, values, shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	if scope.Err() != nil {
		return
	}
	var idx int
	var err error
	if output_indices, idx, err = makeOutputList(op, idx, "output_indices"); err != nil {
		scope.UpdateErr("SparseSplit", err)
		return
	}
	if output_values, idx, err = makeOutputList(op, idx, "output_values"); err != nil {
		scope.UpdateErr("SparseSplit", err)
		return
	}
	if output_shape, idx, err = makeOutputList(op, idx, "output_shape"); err != nil {
		scope.UpdateErr("SparseSplit", err)
		return
	}
	return output_indices, output_values, output_shape
}

// SparseToDenseAttr is an optional argument to SparseToDense.
type SparseToDenseAttr func(optionalAttr)


// SparseToDenseValidateIndices sets the optional validate_indices attribute to value.
//
// value: If true, indices are checked to make sure they are sorted in
// lexicographic order and that there are no repeats.
// If not specified, defaults to true 
func SparseToDenseValidateIndices(value bool) SparseToDenseAttr {
	return func(m optionalAttr) {
		m["validate_indices"] = value
	}
}

// Converts a sparse representation into a dense tensor.
//
// Builds an array `dense` with shape `output_shape` such that
// 
// ```prettyprint
// # If sparse_indices is scalar
// dense[i] = (i == sparse_indices ? sparse_values : default_value)
// 
// # If sparse_indices is a vector, then for each i
// dense[sparse_indices[i]] = sparse_values[i]
// 
// # If sparse_indices is an n by d matrix, then for each i in [0, n)
// dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
// ```
// 
// All other values in `dense` are set to `default_value`.  If `sparse_values` is a
// scalar, all sparse indices are set to this single value.
// 
// Indices should be sorted in lexicographic order, and indices must not
// contain any repeats. If `validate_indices` is true, these properties
// are checked during execution.
//
// Arguments:
//	sparse_indices: 0-D, 1-D, or 2-D.  `sparse_indices[i]` contains the complete
// index where `sparse_values[i]` will be placed.
//	output_shape: 1-D.  Shape of the dense output tensor.
//	sparse_values: 1-D.  Values corresponding to each row of `sparse_indices`,
// or a scalar value to be used for all sparse indices.
//	default_value: Scalar value to set for indices not specified in
// `sparse_indices`.
//
// Returns Dense output tensor of shape `output_shape`.
func SparseToDense(scope *Scope, sparse_indices tf.Output, output_shape tf.Output, sparse_values tf.Output, default_value tf.Output, optional ...SparseToDenseAttr)(dense tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseToDense",
		Input: []tf.Input{
			sparse_indices, output_shape, sparse_values, default_value, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// DenseToSparseSetOperationAttr is an optional argument to DenseToSparseSetOperation.
type DenseToSparseSetOperationAttr func(optionalAttr)


// DenseToSparseSetOperationValidateIndices sets the optional validate_indices attribute to value.
// If not specified, defaults to true 
func DenseToSparseSetOperationValidateIndices(value bool) DenseToSparseSetOperationAttr {
	return func(m optionalAttr) {
		m["validate_indices"] = value
	}
}

// Applies set operation along last dimension of `Tensor` and `SparseTensor`.
//
// See SetOperationOp::SetOperationFromContext for values of `set_operation`.
// 
// Input `set2` is a `SparseTensor` represented by `set2_indices`, `set2_values`,
// and `set2_shape`. For `set2` ranked `n`, 1st `n-1` dimensions must be the same
// as `set1`. Dimension `n` contains values in a set, duplicates are allowed but
// ignored.
// 
// If `validate_indices` is `True`, this op validates the order and range of `set2`
// indices.
// 
// Output `result` is a `SparseTensor` represented by `result_indices`,
// `result_values`, and `result_shape`. For `set1` and `set2` ranked `n`, this
// has rank `n` and the same 1st `n-1` dimensions as `set1` and `set2`. The `nth`
// dimension contains the result of `set_operation` applied to the corresponding
// `[0...n-1]` dimension of `set`.
//
// Arguments:
//	set1: `Tensor` with rank `n`. 1st `n-1` dimensions must be the same as `set2`.
// Dimension `n` contains values in a set, duplicates are allowed but ignored.
//	set2_indices: 2D `Tensor`, indices of a `SparseTensor`. Must be in row-major
// order.
//	set2_values: 1D `Tensor`, values of a `SparseTensor`. Must be in row-major
// order.
//	set2_shape: 1D `Tensor`, shape of a `SparseTensor`. `set2_shape[0...n-1]` must
// be the same as the 1st `n-1` dimensions of `set1`, `result_shape[n]` is the
// max set size across `n-1` dimensions.
//	
//
// Returns 2D indices of a `SparseTensor`.1D values of a `SparseTensor`.1D `Tensor` shape of a `SparseTensor`. `result_shape[0...n-1]` is
// the same as the 1st `n-1` dimensions of `set1` and `set2`, `result_shape[n]`
// is the max result set size across all `0...n-1` dimensions.
func DenseToSparseSetOperation(scope *Scope, set1 tf.Output, set2_indices tf.Output, set2_values tf.Output, set2_shape tf.Output, set_operation string, optional ...DenseToSparseSetOperationAttr)(result_indices tf.Output, result_values tf.Output, result_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"set_operation": set_operation,}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "DenseToSparseSetOperation",
		Input: []tf.Input{
			set1, set2_indices, set2_values, set2_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// SparseTensorDenseMatMulAttr is an optional argument to SparseTensorDenseMatMul.
type SparseTensorDenseMatMulAttr func(optionalAttr)


// SparseTensorDenseMatMulAdjointA sets the optional adjoint_a attribute to value.
//
// value: Use the adjoint of A in the matrix multiply.  If A is complex, this
// is transpose(conj(A)).  Otherwise it's transpose(A).
// If not specified, defaults to false 
func SparseTensorDenseMatMulAdjointA(value bool) SparseTensorDenseMatMulAttr {
	return func(m optionalAttr) {
		m["adjoint_a"] = value
	}
}

// SparseTensorDenseMatMulAdjointB sets the optional adjoint_b attribute to value.
//
// value: Use the adjoint of B in the matrix multiply.  If B is complex, this
// is transpose(conj(B)).  Otherwise it's transpose(B).
// If not specified, defaults to false 
func SparseTensorDenseMatMulAdjointB(value bool) SparseTensorDenseMatMulAttr {
	return func(m optionalAttr) {
		m["adjoint_b"] = value
	}
}

// Multiply SparseTensor (of rank 2) "A" by dense matrix "B".
//
// No validity checking is performed on the indices of A.  However, the following
// input format is recommended for optimal behavior:
// 
// if adjoint_a == false:
//   A should be sorted in lexicographically increasing order.  Use SparseReorder
//   if you're not sure.
// if adjoint_a == true:
//   A should be sorted in order of increasing dimension 1 (i.e., "column major"
//   order instead of "row major" order).
//
// Arguments:
//	a_indices: 2-D.  The `indices` of the `SparseTensor`, size `[nnz, 2]` Matrix.
//	a_values: 1-D.  The `values` of the `SparseTensor`, size `[nnz]` Vector.
//	a_shape: 1-D.  The `shape` of the `SparseTensor`, size `[2]` Vector.
//	b: 2-D.  A dense Matrix.
func SparseTensorDenseMatMul(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b tf.Output, optional ...SparseTensorDenseMatMulAttr)(product tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseTensorDenseMatMul",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Deserialize and concatenate `SparseTensors` from a serialized minibatch.
//
// The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where
// `N` is the minibatch size and the rows correspond to packed outputs of
// `SerializeSparse`.  The ranks of the original `SparseTensor` objects
// must all match.  When the final `SparseTensor` is created, it has rank one
// higher than the ranks of the incoming `SparseTensor` objects
// (they have been concatenated along a new row dimension).
// 
// The output `SparseTensor` object's shape values for all dimensions but the
// first are the max across the input `SparseTensor` objects' shape values
// for the corresponding dimensions.  Its first shape value is `N`, the minibatch
// size.
// 
// The input `SparseTensor` objects' indices are assumed ordered in
// standard lexicographic order.  If this is not the case, after this
// step run `SparseReorder` to restore index ordering.
// 
// For example, if the serialized input is a `[2 x 3]` matrix representing two
// original `SparseTensor` objects:
// 
//     index = [ 0]
//             [10]
//             [20]
//     values = [1, 2, 3]
//     shape = [50]
// 
// and
// 
//     index = [ 2]
//             [10]
//     values = [4, 5]
//     shape = [30]
// 
// then the final deserialized `SparseTensor` will be:
// 
//     index = [0  0]
//             [0 10]
//             [0 20]
//             [1  2]
//             [1 10]
//     values = [1, 2, 3, 4, 5]
//     shape = [2 50]
//
// Arguments:
//	serialized_sparse: 2-D, The `N` serialized `SparseTensor` objects.
// Must have 3 columns.
//	dtype: The `dtype` of the serialized `SparseTensor` objects.
func DeserializeManySparse(scope *Scope, serialized_sparse tf.Output, dtype tf.DataType)(sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"dtype": dtype,}
	opspec := tf.OpSpec{
		Type: "DeserializeManySparse",
		Input: []tf.Input{
			serialized_sparse, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// Serialize an `N`-minibatch `SparseTensor` into an `[N, 3]` string `Tensor`.
//
// The `SparseTensor` must have rank `R` greater than 1, and the first dimension
// is treated as the minibatch dimension.  Elements of the `SparseTensor`
// must be sorted in increasing order of this first dimension.  The serialized
// `SparseTensor` objects going into each row of `serialized_sparse` will have
// rank `R-1`.
// 
// The minibatch size `N` is extracted from `sparse_shape[0]`.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the minibatch `SparseTensor`.
//	sparse_values: 1-D.  The `values` of the minibatch `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the minibatch `SparseTensor`.
func SerializeManySparse(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output)(serialized_sparse tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SerializeManySparse",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Component-wise divides a SparseTensor by a dense Tensor.
//
// *Limitation*: this Op only broadcasts the dense side to the sparse side, but not
// the other direction.
//
// Arguments:
//	sp_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	sp_values: 1-D.  `N` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//	dense: `R`-D.  The dense Tensor operand.
//
// Returns 1-D.  The `N` values that are operated on.
func SparseDenseCwiseDiv(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output, dense tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseDenseCwiseDiv",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, dense, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Serialize a `SparseTensor` into a string 3-vector (1-D `Tensor`) object.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the `SparseTensor`.
//	sparse_values: 1-D.  The `values` of the `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the `SparseTensor`.
func SerializeSparse(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output)(serialized_sparse tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SerializeSparse",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseToSparseSetOperationAttr is an optional argument to SparseToSparseSetOperation.
type SparseToSparseSetOperationAttr func(optionalAttr)


// SparseToSparseSetOperationValidateIndices sets the optional validate_indices attribute to value.
// If not specified, defaults to true 
func SparseToSparseSetOperationValidateIndices(value bool) SparseToSparseSetOperationAttr {
	return func(m optionalAttr) {
		m["validate_indices"] = value
	}
}

// Applies set operation along last dimension of 2 `SparseTensor` inputs.
//
// See SetOperationOp::SetOperationFromContext for values of `set_operation`.
// 
// If `validate_indices` is `True`, `SparseToSparseSetOperation` validates the
// order and range of `set1` and `set2` indices.
// 
// Input `set1` is a `SparseTensor` represented by `set1_indices`, `set1_values`,
// and `set1_shape`. For `set1` ranked `n`, 1st `n-1` dimensions must be the same
// as `set2`. Dimension `n` contains values in a set, duplicates are allowed but
// ignored.
// 
// Input `set2` is a `SparseTensor` represented by `set2_indices`, `set2_values`,
// and `set2_shape`. For `set2` ranked `n`, 1st `n-1` dimensions must be the same
// as `set1`. Dimension `n` contains values in a set, duplicates are allowed but
// ignored.
// 
// If `validate_indices` is `True`, this op validates the order and range of `set1`
// and `set2` indices.
// 
// Output `result` is a `SparseTensor` represented by `result_indices`,
// `result_values`, and `result_shape`. For `set1` and `set2` ranked `n`, this
// has rank `n` and the same 1st `n-1` dimensions as `set1` and `set2`. The `nth`
// dimension contains the result of `set_operation` applied to the corresponding
// `[0...n-1]` dimension of `set`.
//
// Arguments:
//	set1_indices: 2D `Tensor`, indices of a `SparseTensor`. Must be in row-major
// order.
//	set1_values: 1D `Tensor`, values of a `SparseTensor`. Must be in row-major
// order.
//	set1_shape: 1D `Tensor`, shape of a `SparseTensor`. `set1_shape[0...n-1]` must
// be the same as `set2_shape[0...n-1]`, `set1_shape[n]` is the
// max set size across `0...n-1` dimensions.
//	set2_indices: 2D `Tensor`, indices of a `SparseTensor`. Must be in row-major
// order.
//	set2_values: 1D `Tensor`, values of a `SparseTensor`. Must be in row-major
// order.
//	set2_shape: 1D `Tensor`, shape of a `SparseTensor`. `set2_shape[0...n-1]` must
// be the same as `set1_shape[0...n-1]`, `set2_shape[n]` is the
// max set size across `0...n-1` dimensions.
//	
//
// Returns 2D indices of a `SparseTensor`.1D values of a `SparseTensor`.1D `Tensor` shape of a `SparseTensor`. `result_shape[0...n-1]` is
// the same as the 1st `n-1` dimensions of `set1` and `set2`, `result_shape[n]`
// is the max result set size across all `0...n-1` dimensions.
func SparseToSparseSetOperation(scope *Scope, set1_indices tf.Output, set1_values tf.Output, set1_shape tf.Output, set2_indices tf.Output, set2_values tf.Output, set2_shape tf.Output, set_operation string, optional ...SparseToSparseSetOperationAttr)(result_indices tf.Output, result_values tf.Output, result_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"set_operation": set_operation,}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseToSparseSetOperation",
		Input: []tf.Input{
			set1_indices, set1_values, set1_shape, set2_indices, set2_values, set2_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// AddSparseToTensorsMapAttr is an optional argument to AddSparseToTensorsMap.
type AddSparseToTensorsMapAttr func(optionalAttr)


// AddSparseToTensorsMapContainer sets the optional container attribute to value.
//
// value: The container name for the `SparseTensorsMap` created by this op.
// If not specified, defaults to "" 
func AddSparseToTensorsMapContainer(value string) AddSparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["container"] = value
	}
}

// AddSparseToTensorsMapSharedName sets the optional shared_name attribute to value.
//
// value: The shared name for the `SparseTensorsMap` created by this op.
// If blank, the new Operation's unique name is used.
// If not specified, defaults to "" 
func AddSparseToTensorsMapSharedName(value string) AddSparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["shared_name"] = value
	}
}

// Add a `SparseTensor` to a `SparseTensorsMap` return its handle.
//
// A `SparseTensor` is represented by three tensors: `sparse_indices`,
// `sparse_values`, and `sparse_shape`.
// 
// This operator takes the given `SparseTensor` and adds it to a container
// object (a `SparseTensorsMap`).  A unique key within this container is generated
// in the form of an `int64`, and this is the value that is returned.
// 
// The `SparseTensor` can then be read out as part of a minibatch by passing
// the key as a vector element to `TakeManySparseFromTensorsMap`.  To ensure
// the correct `SparseTensorsMap` is accessed, ensure that the same
// `container` and `shared_name` are passed to that Op.  If no `shared_name`
// is provided here, instead use the *name* of the Operation created by calling
// `AddSparseToTensorsMap` as the `shared_name` passed to
// `TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the `SparseTensor`.
//	sparse_values: 1-D.  The `values` of the `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the `SparseTensor`.
//
// Returns 0-D.  The handle of the `SparseTensor` now stored in the
// `SparseTensorsMap`.
func AddSparseToTensorsMap(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output, optional ...AddSparseToTensorsMapAttr)(sparse_handle tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "AddSparseToTensorsMap",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes gradients for SparseSegmentMean.
//
// Returns tensor "output" with same shape as grad, except for dimension 0 whose
// value is output_dim0.
//
// Arguments:
//	grad: gradient propagated to the SparseSegmentMean op.
//	indices: indices passed to the corresponding SparseSegmentMean op.
//	segment_ids: segment_ids passed to the corresponding SparseSegmentMean op.
//	output_dim0: dimension 0 of "data" passed to SparseSegmentMean op.
func SparseSegmentMeanGrad(scope *Scope, grad tf.Output, indices tf.Output, segment_ids tf.Output, output_dim0 tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentMeanGrad",
		Input: []tf.Input{
			grad, indices, segment_ids, output_dim0, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// AddManySparseToTensorsMapAttr is an optional argument to AddManySparseToTensorsMap.
type AddManySparseToTensorsMapAttr func(optionalAttr)


// AddManySparseToTensorsMapContainer sets the optional container attribute to value.
//
// value: The container name for the `SparseTensorsMap` created by this op.
// If not specified, defaults to "" 
func AddManySparseToTensorsMapContainer(value string) AddManySparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["container"] = value
	}
}

// AddManySparseToTensorsMapSharedName sets the optional shared_name attribute to value.
//
// value: The shared name for the `SparseTensorsMap` created by this op.
// If blank, the new Operation's unique name is used.
// If not specified, defaults to "" 
func AddManySparseToTensorsMapSharedName(value string) AddManySparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["shared_name"] = value
	}
}

// Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.
//
// A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
// `sparse_values`, and `sparse_shape`, where
// 
// ```sparse_indices.shape[1] == sparse_shape.shape[0] == R```
// 
// An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
// having a first `sparse_indices` column taking values between `[0, N)`, where
// the minibatch size `N == sparse_shape[0]`.
// 
// The input `SparseTensor` must have rank `R` greater than 1, and the first
// dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
// must be sorted in increasing order of this first dimension.  The stored
// `SparseTensor` objects pointed to by each row of the output `sparse_handles`
// will have rank `R-1`.
// 
// The `SparseTensor` values can then be read out as part of a minibatch by passing
// the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
// the correct `SparseTensorsMap` is accessed, ensure that the same
// `container` and `shared_name` are passed to that Op.  If no `shared_name`
// is provided here, instead use the *name* of the Operation created by calling
// `AddManySparseToTensorsMap` as the `shared_name` passed to
// `TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the minibatch `SparseTensor`.
// `sparse_indices[:, 0]` must be ordered values in `[0, N)`.
//	sparse_values: 1-D.  The `values` of the minibatch `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the minibatch `SparseTensor`.
// The minibatch size `N == sparse_shape[0]`.
//
// Returns 1-D.  The handles of the `SparseTensor` now stored in the
// `SparseTensorsMap`.  Shape: `[N]`.
func AddManySparseToTensorsMap(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output, optional ...AddManySparseToTensorsMapAttr)(sparse_handles tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "AddManySparseToTensorsMap",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseReduceSumAttr is an optional argument to SparseReduceSum.
type SparseReduceSumAttr func(optionalAttr)


// SparseReduceSumKeepDims sets the optional keep_dims attribute to value.
//
// value: If true, retain reduced dimensions with length 1.
// If not specified, defaults to false 
func SparseReduceSumKeepDims(value bool) SparseReduceSumAttr {
	return func(m optionalAttr) {
		m["keep_dims"] = value
	}
}

// Computes the sum of elements across dimensions of a SparseTensor.
//
// This Op takes a SparseTensor and is the sparse counterpart to
// `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`
// instead of a sparse one.
// 
// Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
// `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
// `reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
// with length 1.
// 
// If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
// with a single element is returned.  Additionally, the axes can be negative,
// which are interpreted according to the indexing rules in Python.
//
// Arguments:
//	input_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	input_values: 1-D.  `N` non-empty values corresponding to `input_indices`.
//	input_shape: 1-D.  Shape of the input SparseTensor.
//	reduction_axes: 1-D.  Length-`K` vector containing the reduction axes.
//
// Returns `R-K`-D.  The reduced Tensor.
func SparseReduceSum(scope *Scope, input_indices tf.Output, input_values tf.Output, input_shape tf.Output, reduction_axes tf.Output, optional ...SparseReduceSumAttr)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseReduceSum",
		Input: []tf.Input{
			input_indices, input_values, input_shape, reduction_axes, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Returns the element-wise max of two SparseTensors.
//
// Assumes the two SparseTensors have the same shape, i.e., no broadcasting.
//
// Arguments:
//	a_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, in the canonical lexicographic ordering.
//	a_values: 1-D.  `N` non-empty values corresponding to `a_indices`.
//	a_shape: 1-D.  Shape of the input SparseTensor.
//	b_indices: counterpart to `a_indices` for the other operand.
//	b_values: counterpart to `a_values` for the other operand; must be of the same dtype.
//	b_shape: counterpart to `a_shape` for the other operand; the two shapes must be equal.
//
// Returns 2-D.  The indices of the output SparseTensor.1-D.  The values of the output SparseTensor.
func SparseSparseMaximum(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b_indices tf.Output, b_values tf.Output, b_shape tf.Output)(output_indices tf.Output, output_values tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSparseMaximum",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b_indices, b_values, b_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// The gradient operator for the SparseAdd op.
//
// The SparseAdd op calculates A + B, where A, B, and the sum are all represented
// as `SparseTensor` objects.  This op takes in the upstream gradient w.r.t.
// non-empty values of the sum, and outputs the gradients w.r.t. the non-empty
// values of A and B.
//
// Arguments:
//	backprop_val_grad: 1-D with shape `[nnz(sum)]`.  The gradient with respect to
// the non-empty values of the sum.
//	a_indices: 2-D.  The `indices` of the `SparseTensor` A, size `[nnz(A), ndims]`.
//	b_indices: 2-D.  The `indices` of the `SparseTensor` B, size `[nnz(B), ndims]`.
//	sum_indices: 2-D.  The `indices` of the sum `SparseTensor`, size
// `[nnz(sum), ndims]`.
//
// Returns 1-D with shape `[nnz(A)]`. The gradient with respect to the
// non-empty values of A.1-D with shape `[nnz(B)]`. The gradient with respect to the
// non-empty values of B.
func SparseAddGrad(scope *Scope, backprop_val_grad tf.Output, a_indices tf.Output, b_indices tf.Output, sum_indices tf.Output)(a_val_grad tf.Output, b_val_grad tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseAddGrad",
		Input: []tf.Input{
			backprop_val_grad, a_indices, b_indices, sum_indices, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Computes the mean along sparse segments of a tensor.
//
// Read @{$math_ops#segmentation$the section on segmentation} for an explanation of
// segments.
// 
// Like `SegmentMean`, but `segment_ids` can have rank less than `data`'s first
// dimension, selecting a subset of dimension 0, specified by `indices`.
//
// Arguments:
//	
//	indices: A 1-D tensor. Has same rank as `segment_ids`.
//	segment_ids: A 1-D tensor. Values should be sorted and can be repeated.
//
// Returns Has same shape as data, except for dimension 0 which
// has size `k`, the number of segments.
func SparseSegmentMean(scope *Scope, data tf.Output, indices tf.Output, segment_ids tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentMean",
		Input: []tf.Input{
			data, indices, segment_ids, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes the sum along sparse segments of a tensor.
//
// Read @{$math_ops#segmentation$the section on segmentation} for an explanation of
// segments.
// 
// Like `SegmentSum`, but `segment_ids` can have rank less than `data`'s first
// dimension, selecting a subset of dimension 0, specified by `indices`.
// 
// For example:
// 
// ```prettyprint
// c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])
// 
// # Select two rows, one segment.
// tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))
//   ==> [[0 0 0 0]]
// 
// # Select two rows, two segment.
// tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))
//   ==> [[ 1  2  3  4]
//        [-1 -2 -3 -4]]
// 
// # Select all rows, two segments.
// tf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))
//   ==> [[0 0 0 0]
//        [5 6 7 8]]
// 
// # Which is equivalent to:
// tf.segment_sum(c, tf.constant([0, 0, 1]))
// ```
//
// Arguments:
//	
//	indices: A 1-D tensor. Has same rank as `segment_ids`.
//	segment_ids: A 1-D tensor. Values should be sorted and can be repeated.
//
// Returns Has same shape as data, except for dimension 0 which
// has size `k`, the number of segments.
func SparseSegmentSum(scope *Scope, data tf.Output, indices tf.Output, segment_ids tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentSum",
		Input: []tf.Input{
			data, indices, segment_ids, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseMatMulAttr is an optional argument to SparseMatMul.
type SparseMatMulAttr func(optionalAttr)


// SparseMatMulTransposeA sets the optional transpose_a attribute to value.
// If not specified, defaults to false 
func SparseMatMulTransposeA(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["transpose_a"] = value
	}
}

// SparseMatMulTransposeB sets the optional transpose_b attribute to value.
// If not specified, defaults to false 
func SparseMatMulTransposeB(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["transpose_b"] = value
	}
}

// SparseMatMulAIsSparse sets the optional a_is_sparse attribute to value.
// If not specified, defaults to false 
func SparseMatMulAIsSparse(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["a_is_sparse"] = value
	}
}

// SparseMatMulBIsSparse sets the optional b_is_sparse attribute to value.
// If not specified, defaults to false 
func SparseMatMulBIsSparse(value bool) SparseMatMulAttr {
	return func(m optionalAttr) {
		m["b_is_sparse"] = value
	}
}

// Multiply matrix "a" by matrix "b".
//
// The inputs must be two-dimensional matrices and the inner dimension of "a" must
// match the outer dimension of "b". This op is optimized for the case where at
// least one of "a" or "b" is sparse. The breakeven for using this versus a dense
// matrix multiply on one platform was 30% zero values in the sparse matrix.
func SparseMatMul(scope *Scope, a tf.Output, b tf.Output, optional ...SparseMatMulAttr)(product tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseMatMul",
		Input: []tf.Input{
			a, b, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes gradients for SparseSegmentSqrtN.
//
// Returns tensor "output" with same shape as grad, except for dimension 0 whose
// value is output_dim0.
//
// Arguments:
//	grad: gradient propagated to the SparseSegmentSqrtN op.
//	indices: indices passed to the corresponding SparseSegmentSqrtN op.
//	segment_ids: segment_ids passed to the corresponding SparseSegmentSqrtN op.
//	output_dim0: dimension 0 of "data" passed to SparseSegmentSqrtN op.
func SparseSegmentSqrtNGrad(scope *Scope, grad tf.Output, indices tf.Output, segment_ids tf.Output, output_dim0 tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentSqrtNGrad",
		Input: []tf.Input{
			grad, indices, segment_ids, output_dim0, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes the sum along sparse segments of a tensor divided by the sqrt of N.
//
// N is the size of the segment being reduced.
// 
// Read @{$math_ops#segmentation$the section on segmentation} for an explanation of
// segments.
//
// Arguments:
//	
//	indices: A 1-D tensor. Has same rank as `segment_ids`.
//	segment_ids: A 1-D tensor. Values should be sorted and can be repeated.
//
// Returns Has same shape as data, except for dimension 0 which
// has size `k`, the number of segments.
func SparseSegmentSqrtN(scope *Scope, data tf.Output, indices tf.Output, segment_ids tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentSqrtN",
		Input: []tf.Input{
			data, indices, segment_ids, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseReduceSumSparseAttr is an optional argument to SparseReduceSumSparse.
type SparseReduceSumSparseAttr func(optionalAttr)


// SparseReduceSumSparseKeepDims sets the optional keep_dims attribute to value.
//
// value: If true, retain reduced dimensions with length 1.
// If not specified, defaults to false 
func SparseReduceSumSparseKeepDims(value bool) SparseReduceSumSparseAttr {
	return func(m optionalAttr) {
		m["keep_dims"] = value
	}
}

// Computes the sum of elements across dimensions of a SparseTensor.
//
// This Op takes a SparseTensor and is the sparse counterpart to
// `tf.reduce_sum()`.  In contrast to SparseReduceSum, this Op returns a
// SparseTensor.
// 
// Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
// `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
// `reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
// with length 1.
// 
// If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
// with a single element is returned.  Additionally, the axes can be negative,
// which are interpreted according to the indexing rules in Python.
//
// Arguments:
//	input_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	input_values: 1-D.  `N` non-empty values corresponding to `input_indices`.
//	input_shape: 1-D.  Shape of the input SparseTensor.
//	reduction_axes: 1-D.  Length-`K` vector containing the reduction axes.
func SparseReduceSumSparse(scope *Scope, input_indices tf.Output, input_values tf.Output, input_shape tf.Output, reduction_axes tf.Output, optional ...SparseReduceSumSparseAttr)(output_indices tf.Output, output_values tf.Output, output_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseReduceSumSparse",
		Input: []tf.Input{
			input_indices, input_values, input_shape, reduction_axes, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// ResourceSparseApplyRMSPropAttr is an optional argument to ResourceSparseApplyRMSProp.
type ResourceSparseApplyRMSPropAttr func(optionalAttr)


// ResourceSparseApplyRMSPropUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var, ms, and mom tensors is protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyRMSPropUseLocking(value bool) ResourceSparseApplyRMSPropAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update '*var' according to the RMSProp algorithm.
//
// Note that in dense implementation of this algorithm, ms and mom will
// update even if the grad is zero, but in this sparse implementation, ms
// and mom will not update in iterations during which the grad is zero.
// 
// mean_square = decay * mean_square + (1-decay) * gradient ** 2
// Delta = learning_rate * gradient / sqrt(mean_square + epsilon)
// 
// ms <- rho * ms_{t-1} + (1-rho) * grad * grad
// mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
// var <- var - mom
//
// Arguments:
//	var_: Should be from a Variable().
//	ms: Should be from a Variable().
//	mom: Should be from a Variable().
//	lr: Scaling factor. Must be a scalar.
//	rho: Decay rate. Must be a scalar.
//	
//	epsilon: Ridge term. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var, ms and mom.
//
// Returns the created operation.
func ResourceSparseApplyRMSProp(scope *Scope, var_ tf.Output, ms tf.Output, mom tf.Output, lr tf.Output, rho tf.Output, momentum tf.Output, epsilon tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyRMSPropAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyRMSProp",
		Input: []tf.Input{
			var_, ms, mom, lr, rho, momentum, epsilon, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// Adds two `SparseTensor` objects to produce another `SparseTensor`.
//
// The input `SparseTensor` objects' indices are assumed ordered in standard
// lexicographic order.  If this is not the case, before this step run
// `SparseReorder` to restore index ordering.
// 
// By default, if two values sum to zero at some index, the output `SparseTensor`
// would still include that particular location in its index, storing a zero in the
// corresponding value slot.  To override this, callers can specify `thresh`,
// indicating that if the sum has a magnitude strictly smaller than `thresh`, its
// corresponding value and index would then not be included.  In particular,
// `thresh == 0` (default) means everything is kept and actual thresholding happens
// only for a positive value.
// 
// In the following shapes, `nnz` is the count after taking `thresh` into account.
//
// Arguments:
//	a_indices: 2-D.  The `indices` of the first `SparseTensor`, size `[nnz, ndims]` Matrix.
//	a_values: 1-D.  The `values` of the first `SparseTensor`, size `[nnz]` Vector.
//	a_shape: 1-D.  The `shape` of the first `SparseTensor`, size `[ndims]` Vector.
//	b_indices: 2-D.  The `indices` of the second `SparseTensor`, size `[nnz, ndims]` Matrix.
//	b_values: 1-D.  The `values` of the second `SparseTensor`, size `[nnz]` Vector.
//	b_shape: 1-D.  The `shape` of the second `SparseTensor`, size `[ndims]` Vector.
//	thresh: 0-D.  The magnitude threshold that determines if an output value/index
// pair takes space.
func SparseAdd(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b_indices tf.Output, b_values tf.Output, b_shape tf.Output, thresh tf.Output)(sum_indices tf.Output, sum_values tf.Output, sum_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseAdd",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b_indices, b_values, b_shape, thresh, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// ResourceSparseApplyMomentumAttr is an optional argument to ResourceSparseApplyMomentum.
type ResourceSparseApplyMomentumAttr func(optionalAttr)


// ResourceSparseApplyMomentumUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var and accum tensors will be protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyMomentumUseLocking(value bool) ResourceSparseApplyMomentumAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// ResourceSparseApplyMomentumUseNesterov sets the optional use_nesterov attribute to value.
//
// value: If `True`, the tensor passed to compute grad will be
// var - lr * momentum * accum, so in the end, the var you get is actually
// var - lr * momentum * accum.
// If not specified, defaults to false 
func ResourceSparseApplyMomentumUseNesterov(value bool) ResourceSparseApplyMomentumAttr {
	return func(m optionalAttr) {
		m["use_nesterov"] = value
	}
}

// Update relevant entries in '*var' and '*accum' according to the momentum scheme.
//
// Set use_nesterov = True if you want to use Nesterov momentum.
// 
// That is for rows we have grad for, we update var and accum as follows:
// 
// accum = accum * momentum + grad
// var -= lr * accum
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//	momentum: Momentum. Must be a scalar.
//
// Returns the created operation.
func ResourceSparseApplyMomentum(scope *Scope, var_ tf.Output, accum tf.Output, lr tf.Output, grad tf.Output, indices tf.Output, momentum tf.Output, optional ...ResourceSparseApplyMomentumAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyMomentum",
		Input: []tf.Input{
			var_, accum, lr, grad, indices, momentum, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyProximalGradientDescentAttr is an optional argument to ResourceSparseApplyProximalGradientDescent.
type ResourceSparseApplyProximalGradientDescentAttr func(optionalAttr)


// ResourceSparseApplyProximalGradientDescentUseLocking sets the optional use_locking attribute to value.
//
// value: If True, the subtraction will be protected by a lock;
// otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyProximalGradientDescentUseLocking(value bool) ResourceSparseApplyProximalGradientDescentAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Sparse update '*var' as FOBOS algorithm with fixed learning rate.
//
// That is for rows we have grad for, we update var as follows:
// prox_v = var - alpha * grad
// var = sign(prox_v)/(1+alpha*l2) * max{|prox_v|-alpha*l1,0}
//
// Arguments:
//	var_: Should be from a Variable().
//	alpha: Scaling factor. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyProximalGradientDescent(scope *Scope, var_ tf.Output, alpha tf.Output, l1 tf.Output, l2 tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyProximalGradientDescentAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyProximalGradientDescent",
		Input: []tf.Input{
			var_, alpha, l1, l2, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyProximalAdagradAttr is an optional argument to ResourceSparseApplyProximalAdagrad.
type ResourceSparseApplyProximalAdagradAttr func(optionalAttr)


// ResourceSparseApplyProximalAdagradUseLocking sets the optional use_locking attribute to value.
//
// value: If True, updating of the var and accum tensors will be protected by
// a lock; otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyProximalAdagradUseLocking(value bool) ResourceSparseApplyProximalAdagradAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Sparse update entries in '*var' and '*accum' according to FOBOS algorithm.
//
// That is for rows we have grad for, we update var and accum as follows:
// accum += grad * grad
// prox_v = var
// prox_v -= lr * grad * (1 / sqrt(accum))
// var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyProximalAdagrad(scope *Scope, var_ tf.Output, accum tf.Output, lr tf.Output, l1 tf.Output, l2 tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyProximalAdagradAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyProximalAdagrad",
		Input: []tf.Input{
			var_, accum, lr, l1, l2, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyAdagradDAAttr is an optional argument to ResourceSparseApplyAdagradDA.
type ResourceSparseApplyAdagradDAAttr func(optionalAttr)


// ResourceSparseApplyAdagradDAUseLocking sets the optional use_locking attribute to value.
//
// value: If True, updating of the var and accum tensors will be protected by
// a lock; otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyAdagradDAUseLocking(value bool) ResourceSparseApplyAdagradDAAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update entries in '*var' and '*accum' according to the proximal adagrad scheme.
//
// Arguments:
//	var_: Should be from a Variable().
//	gradient_accumulator: Should be from a Variable().
//	gradient_squared_accumulator: Should be from a Variable().
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//	lr: Learning rate. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	global_step: Training step number. Must be a scalar.
//
// Returns the created operation.
func ResourceSparseApplyAdagradDA(scope *Scope, var_ tf.Output, gradient_accumulator tf.Output, gradient_squared_accumulator tf.Output, grad tf.Output, indices tf.Output, lr tf.Output, l1 tf.Output, l2 tf.Output, global_step tf.Output, optional ...ResourceSparseApplyAdagradDAAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyAdagradDA",
		Input: []tf.Input{
			var_, gradient_accumulator, gradient_squared_accumulator, grad, indices, lr, l1, l2, global_step, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyCenteredRMSPropAttr is an optional argument to ResourceSparseApplyCenteredRMSProp.
type ResourceSparseApplyCenteredRMSPropAttr func(optionalAttr)


// ResourceSparseApplyCenteredRMSPropUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var, mg, ms, and mom tensors is
// protected by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyCenteredRMSPropUseLocking(value bool) ResourceSparseApplyCenteredRMSPropAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update '*var' according to the centered RMSProp algorithm.
//
// The centered RMSProp algorithm uses an estimate of the centered second moment
// (i.e., the variance) for normalization, as opposed to regular RMSProp, which
// uses the (uncentered) second moment. This often helps with training, but is
// slightly more expensive in terms of computation and memory.
// 
// Note that in dense implementation of this algorithm, mg, ms, and mom will
// update even if the grad is zero, but in this sparse implementation, mg, ms,
// and mom will not update in iterations during which the grad is zero.
// 
// mean_square = decay * mean_square + (1-decay) * gradient ** 2
// mean_grad = decay * mean_grad + (1-decay) * gradient
// Delta = learning_rate * gradient / sqrt(mean_square + epsilon - mean_grad ** 2)
// 
// ms <- rho * ms_{t-1} + (1-rho) * grad * grad
// mom <- momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon)
// var <- var - mom
//
// Arguments:
//	var_: Should be from a Variable().
//	mg: Should be from a Variable().
//	ms: Should be from a Variable().
//	mom: Should be from a Variable().
//	lr: Scaling factor. Must be a scalar.
//	rho: Decay rate. Must be a scalar.
//	
//	epsilon: Ridge term. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var, ms and mom.
//
// Returns the created operation.
func ResourceSparseApplyCenteredRMSProp(scope *Scope, var_ tf.Output, mg tf.Output, ms tf.Output, mom tf.Output, lr tf.Output, rho tf.Output, momentum tf.Output, epsilon tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyCenteredRMSPropAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyCenteredRMSProp",
		Input: []tf.Input{
			var_, mg, ms, mom, lr, rho, momentum, epsilon, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyAdadeltaAttr is an optional argument to ResourceSparseApplyAdadelta.
type ResourceSparseApplyAdadeltaAttr func(optionalAttr)


// ResourceSparseApplyAdadeltaUseLocking sets the optional use_locking attribute to value.
//
// value: If True, updating of the var and accum tensors will be protected by
// a lock; otherwise the behavior is undefined, but may exhibit less contention.
// If not specified, defaults to false 
func ResourceSparseApplyAdadeltaUseLocking(value bool) ResourceSparseApplyAdadeltaAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// var: Should be from a Variable().
//
// Arguments:
//	
//	accum: Should be from a Variable().
//	accum_update: : Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	rho: Decay factor. Must be a scalar.
//	epsilon: Constant factor. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyAdadelta(scope *Scope, var_ tf.Output, accum tf.Output, accum_update tf.Output, lr tf.Output, rho tf.Output, epsilon tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyAdadeltaAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyAdadelta",
		Input: []tf.Input{
			var_, accum, accum_update, lr, rho, epsilon, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// TakeManySparseFromTensorsMapAttr is an optional argument to TakeManySparseFromTensorsMap.
type TakeManySparseFromTensorsMapAttr func(optionalAttr)


// TakeManySparseFromTensorsMapContainer sets the optional container attribute to value.
//
// value: The container name for the `SparseTensorsMap` read by this op.
// If not specified, defaults to "" 
func TakeManySparseFromTensorsMapContainer(value string) TakeManySparseFromTensorsMapAttr {
	return func(m optionalAttr) {
		m["container"] = value
	}
}

// TakeManySparseFromTensorsMapSharedName sets the optional shared_name attribute to value.
//
// value: The shared name for the `SparseTensorsMap` read by this op.
// It should not be blank; rather the `shared_name` or unique Operation name
// of the Op that created the original `SparseTensorsMap` should be used.
// If not specified, defaults to "" 
func TakeManySparseFromTensorsMapSharedName(value string) TakeManySparseFromTensorsMapAttr {
	return func(m optionalAttr) {
		m["shared_name"] = value
	}
}

// Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.
//
// The input `sparse_handles` must be an `int64` matrix of shape `[N, 1]` where
// `N` is the minibatch size and the rows correspond to the output handles of
// `AddSparseToTensorsMap` or `AddManySparseToTensorsMap`.  The ranks of the
// original `SparseTensor` objects that went into the given input ops must all
// match.  When the final `SparseTensor` is created, it has rank one
// higher than the ranks of the incoming `SparseTensor` objects
// (they have been concatenated along a new row dimension on the left).
// 
// The output `SparseTensor` object's shape values for all dimensions but the
// first are the max across the input `SparseTensor` objects' shape values
// for the corresponding dimensions.  Its first shape value is `N`, the minibatch
// size.
// 
// The input `SparseTensor` objects' indices are assumed ordered in
// standard lexicographic order.  If this is not the case, after this
// step run `SparseReorder` to restore index ordering.
// 
// For example, if the handles represent an input, which is a `[2, 3]` matrix
// representing two original `SparseTensor` objects:
// 
// ```
//     index = [ 0]
//             [10]
//             [20]
//     values = [1, 2, 3]
//     shape = [50]
// ```
// 
// and
// 
// ```
//     index = [ 2]
//             [10]
//     values = [4, 5]
//     shape = [30]
// ```
// 
// then the final `SparseTensor` will be:
// 
// ```
//     index = [0  0]
//             [0 10]
//             [0 20]
//             [1  2]
//             [1 10]
//     values = [1, 2, 3, 4, 5]
//     shape = [2 50]
// ```
//
// Arguments:
//	sparse_handles: 1-D, The `N` serialized `SparseTensor` objects.
// Shape: `[N]`.
//	dtype: The `dtype` of the `SparseTensor` objects stored in the
// `SparseTensorsMap`.
//
// Returns 2-D.  The `indices` of the minibatch `SparseTensor`.1-D.  The `values` of the minibatch `SparseTensor`.1-D.  The `shape` of the minibatch `SparseTensor`.
func TakeManySparseFromTensorsMap(scope *Scope, sparse_handles tf.Output, dtype tf.DataType, optional ...TakeManySparseFromTensorsMapAttr)(sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"dtype": dtype,}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "TakeManySparseFromTensorsMap",
		Input: []tf.Input{
			sparse_handles, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// ResourceSparseApplyFtrlAttr is an optional argument to ResourceSparseApplyFtrl.
type ResourceSparseApplyFtrlAttr func(optionalAttr)


// ResourceSparseApplyFtrlUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var and accum tensors will be protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyFtrlUseLocking(value bool) ResourceSparseApplyFtrlAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update relevant entries in '*var' according to the Ftrl-proximal scheme.
//
// That is for rows we have grad for, we update var, accum and linear as follows:
// accum_new = accum + grad * grad
// linear += grad + (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var
// quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2
// var = (sign(linear) * l1 - linear) / quadratic if |linear| > l1 else 0.0
// accum = accum_new
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	linear: Should be from a Variable().
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//	lr: Scaling factor. Must be a scalar.
//	l1: L1 regularization. Must be a scalar.
//	l2: L2 regularization. Must be a scalar.
//	lr_power: Scaling factor. Must be a scalar.
//
// Returns the created operation.
func ResourceSparseApplyFtrl(scope *Scope, var_ tf.Output, accum tf.Output, linear tf.Output, grad tf.Output, indices tf.Output, lr tf.Output, l1 tf.Output, l2 tf.Output, lr_power tf.Output, optional ...ResourceSparseApplyFtrlAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyFtrl",
		Input: []tf.Input{
			var_, accum, linear, grad, indices, lr, l1, l2, lr_power, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// ResourceSparseApplyAdagradAttr is an optional argument to ResourceSparseApplyAdagrad.
type ResourceSparseApplyAdagradAttr func(optionalAttr)


// ResourceSparseApplyAdagradUseLocking sets the optional use_locking attribute to value.
//
// value: If `True`, updating of the var and accum tensors will be protected
// by a lock; otherwise the behavior is undefined, but may exhibit less
// contention.
// If not specified, defaults to false 
func ResourceSparseApplyAdagradUseLocking(value bool) ResourceSparseApplyAdagradAttr {
	return func(m optionalAttr) {
		m["use_locking"] = value
	}
}

// Update relevant entries in '*var' and '*accum' according to the adagrad scheme.
//
// That is for rows we have grad for, we update var and accum as follows:
// accum += grad * grad
// var -= lr * grad * (1 / sqrt(accum))
//
// Arguments:
//	var_: Should be from a Variable().
//	accum: Should be from a Variable().
//	lr: Learning rate. Must be a scalar.
//	grad: The gradient.
//	indices: A vector of indices into the first dimension of var and accum.
//
// Returns the created operation.
func ResourceSparseApplyAdagrad(scope *Scope, var_ tf.Output, accum tf.Output, lr tf.Output, grad tf.Output, indices tf.Output, optional ...ResourceSparseApplyAdagradAttr)(o *tf.Operation) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "ResourceSparseApplyAdagrad",
		Input: []tf.Input{
			var_, accum, lr, grad, indices, 
		},
		Attrs: attrs,
	}
	return scope.AddOperation(opspec)
}

// Concatenates a list of `SparseTensor` along the specified dimension.
//
// Concatenation is with respect to the dense versions of these sparse tensors.
// It is assumed that each input is a `SparseTensor` whose elements are ordered
// along increasing dimension number.
// 
// All inputs' shapes must match, except for the concat dimension.  The
// `indices`, `values`, and `shapes` lists must have the same length.
// 
// The output shape is identical to the inputs', except along the concat
// dimension, where it is the sum of the inputs' sizes along that dimension.
// 
// The output elements will be resorted to preserve the sort order along
// increasing dimension number.
// 
// This op runs in `O(M log M)` time, where `M` is the total number of non-empty
// values across all inputs. This is due to the need for an internal sort in
// order to concatenate efficiently across an arbitrary dimension.
// 
// For example, if `concat_dim = 1` and the inputs are
// 
//     sp_inputs[0]: shape = [2, 3]
//     [0, 2]: "a"
//     [1, 0]: "b"
//     [1, 1]: "c"
// 
//     sp_inputs[1]: shape = [2, 4]
//     [0, 1]: "d"
//     [0, 2]: "e"
// 
// then the output will be
// 
//     shape = [2, 7]
//     [0, 2]: "a"
//     [0, 4]: "d"
//     [0, 5]: "e"
//     [1, 0]: "b"
//     [1, 1]: "c"
// 
// Graphically this is equivalent to doing
// 
//     [    a] concat [  d e  ] = [    a   d e  ]
//     [b c  ]        [       ]   [b c          ]
//
// Arguments:
//	indices: 2-D.  Indices of each input `SparseTensor`.
//	values: 1-D.  Non-empty values of each `SparseTensor`.
//	shapes: 1-D.  Shapes of each `SparseTensor`.
//	concat_dim: Dimension to concatenate along. Must be in range [-rank, rank),
// where rank is the number of dimensions in each input `SparseTensor`.
//
// Returns 2-D.  Indices of the concatenated `SparseTensor`.1-D.  Non-empty values of the concatenated `SparseTensor`.1-D.  Shape of the concatenated `SparseTensor`.
func SparseConcat(scope *Scope, indices []tf.Output, values []tf.Output, shapes []tf.Output, concat_dim int64)(output_indices tf.Output, output_values tf.Output, output_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"concat_dim": concat_dim,}
	opspec := tf.OpSpec{
		Type: "SparseConcat",
		Input: []tf.Input{
			tf.OutputList(indices), tf.OutputList(values), tf.OutputList(shapes), 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// Returns the element-wise min of two SparseTensors.
//
// Assumes the two SparseTensors have the same shape, i.e., no broadcasting.
//
// Arguments:
//	a_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, in the canonical lexicographic ordering.
//	a_values: 1-D.  `N` non-empty values corresponding to `a_indices`.
//	a_shape: 1-D.  Shape of the input SparseTensor.
//	b_indices: counterpart to `a_indices` for the other operand.
//	b_values: counterpart to `a_values` for the other operand; must be of the same dtype.
//	b_shape: counterpart to `a_shape` for the other operand; the two shapes must be equal.
//
// Returns 2-D.  The indices of the output SparseTensor.1-D.  The values of the output SparseTensor.
func SparseSparseMinimum(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b_indices tf.Output, b_values tf.Output, b_shape tf.Output)(output_indices tf.Output, output_values tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSparseMinimum",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b_indices, b_values, b_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Adds up a SparseTensor and a dense Tensor, using these special rules:
//
// (1) Broadcasts the dense side to have the same shape as the sparse side, if
//     eligible;
// (2) Then, only the dense values pointed to by the indices of the SparseTensor
//     participate in the cwise addition.
// 
// By these rules, the result is a logical SparseTensor with exactly the same
// indices and shape, but possibly with different non-zero values.  The output of
// this Op is the resultant non-zero values.
//
// Arguments:
//	sp_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	sp_values: 1-D.  `N` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//	dense: `R`-D.  The dense Tensor operand.
//
// Returns 1-D.  The `N` values that are operated on.
func SparseDenseCwiseAdd(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output, dense tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseDenseCwiseAdd",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, dense, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Component-wise multiplies a SparseTensor by a dense Tensor.
//
// The output locations corresponding to the implicitly zero elements in the sparse
// tensor will be zero (i.e., will not take up storage space), regardless of the
// contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).
// 
// *Limitation*: this Op only broadcasts the dense side to the sparse side, but not
// the other direction.
//
// Arguments:
//	sp_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	sp_values: 1-D.  `N` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//	dense: `R`-D.  The dense Tensor operand.
//
// Returns 1-D.  The `N` values that are operated on.
func SparseDenseCwiseMul(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output, dense tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseDenseCwiseMul",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, dense, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Adds up a `SparseTensor` and a dense `Tensor`, producing a dense `Tensor`.
//
// This Op does not require `a_indices` be sorted in standard lexicographic order.
//
// Arguments:
//	a_indices: 2-D.  The `indices` of the `SparseTensor`, with shape `[nnz, ndims]`.
//	a_values: 1-D.  The `values` of the `SparseTensor`, with shape `[nnz]`.
//	a_shape: 1-D.  The `shape` of the `SparseTensor`, with shape `[ndims]`.
//	b: `ndims`-D Tensor.  With shape `a_shape`.
func SparseTensorDenseAdd(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseTensorDenseAdd",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Reorders a SparseTensor into the canonical, row-major ordering.
//
// Note that by convention, all sparse ops preserve the canonical ordering along
// increasing dimension number. The only time ordering can be violated is during
// manual manipulation of the indices and values vectors to add entries.
// 
// Reordering does not affect the shape of the SparseTensor.
// 
// If the tensor has rank `R` and `N` non-empty values, `input_indices` has
// shape `[N, R]`, input_values has length `N`, and input_shape has length `R`.
//
// Arguments:
//	input_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	input_values: 1-D.  `N` non-empty values corresponding to `input_indices`.
//	input_shape: 1-D.  Shape of the input SparseTensor.
//
// Returns 2-D.  `N x R` matrix with the same indices as input_indices, but
// in canonical row-major ordering.1-D.  `N` non-empty values corresponding to `output_indices`.
func SparseReorder(scope *Scope, input_indices tf.Output, input_values tf.Output, input_shape tf.Output)(output_indices tf.Output, output_values tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseReorder",
		Input: []tf.Input{
			input_indices, input_values, input_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// Split a `SparseTensor` into `num_split` tensors along one dimension.
//
// If the `shape[split_dim]` is not an integer multiple of `num_split`. Slices
// `[0 : shape[split_dim] % num_split]` gets one extra dimension.
// For example, if `split_dim = 1` and `num_split = 2` and the input is
// 
//     input_tensor = shape = [2, 7]
//     [    a   d e  ]
//     [b c          ]
// 
// Graphically the output tensors are:
// 
//     output_tensor[0] = shape = [2, 4]
//     [    a  ]
//     [b c    ]
// 
//     output_tensor[1] = shape = [2, 3]
//     [ d e  ]
//     [      ]
//
// Arguments:
//	split_dim: 0-D.  The dimension along which to split.  Must be in the range
// `[0, rank(shape))`.
//	indices: 2-D tensor represents the indices of the sparse tensor.
//	values: 1-D tensor represents the values of the sparse tensor.
//	shape: 1-D. tensor represents the shape of the sparse tensor.
// output indices: A list of 1-D tensors represents the indices of the output
// sparse tensors.
//	num_split: The number of ways to split.
//
// Returns A list of 1-D tensors represents the values of the output sparse
// tensors.A list of 1-D tensors represents the shape of the output sparse
// tensors.
func SparseSplit(scope *Scope, split_dim tf.Output, indices tf.Output, values tf.Output, shape tf.Output, num_split int64)(output_indices []tf.Output, output_values []tf.Output, output_shape []tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"num_split": num_split,}
	opspec := tf.OpSpec{
		Type: "SparseSplit",
		Input: []tf.Input{
			split_dim, indices, values, shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	if scope.Err() != nil {
		return
	}
	var idx int
	var err error
	if output_indices, idx, err = makeOutputList(op, idx, "output_indices"); err != nil {
		scope.UpdateErr("SparseSplit", err)
		return
	}
	if output_values, idx, err = makeOutputList(op, idx, "output_values"); err != nil {
		scope.UpdateErr("SparseSplit", err)
		return
	}
	if output_shape, idx, err = makeOutputList(op, idx, "output_shape"); err != nil {
		scope.UpdateErr("SparseSplit", err)
		return
	}
	return output_indices, output_values, output_shape
}

// SparseToDenseAttr is an optional argument to SparseToDense.
type SparseToDenseAttr func(optionalAttr)


// SparseToDenseValidateIndices sets the optional validate_indices attribute to value.
//
// value: If true, indices are checked to make sure they are sorted in
// lexicographic order and that there are no repeats.
// If not specified, defaults to true 
func SparseToDenseValidateIndices(value bool) SparseToDenseAttr {
	return func(m optionalAttr) {
		m["validate_indices"] = value
	}
}

// Converts a sparse representation into a dense tensor.
//
// Builds an array `dense` with shape `output_shape` such that
// 
// ```prettyprint
// # If sparse_indices is scalar
// dense[i] = (i == sparse_indices ? sparse_values : default_value)
// 
// # If sparse_indices is a vector, then for each i
// dense[sparse_indices[i]] = sparse_values[i]
// 
// # If sparse_indices is an n by d matrix, then for each i in [0, n)
// dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
// ```
// 
// All other values in `dense` are set to `default_value`.  If `sparse_values` is a
// scalar, all sparse indices are set to this single value.
// 
// Indices should be sorted in lexicographic order, and indices must not
// contain any repeats. If `validate_indices` is true, these properties
// are checked during execution.
//
// Arguments:
//	sparse_indices: 0-D, 1-D, or 2-D.  `sparse_indices[i]` contains the complete
// index where `sparse_values[i]` will be placed.
//	output_shape: 1-D.  Shape of the dense output tensor.
//	sparse_values: 1-D.  Values corresponding to each row of `sparse_indices`,
// or a scalar value to be used for all sparse indices.
//	default_value: Scalar value to set for indices not specified in
// `sparse_indices`.
//
// Returns Dense output tensor of shape `output_shape`.
func SparseToDense(scope *Scope, sparse_indices tf.Output, output_shape tf.Output, sparse_values tf.Output, default_value tf.Output, optional ...SparseToDenseAttr)(dense tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseToDense",
		Input: []tf.Input{
			sparse_indices, output_shape, sparse_values, default_value, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// DenseToSparseSetOperationAttr is an optional argument to DenseToSparseSetOperation.
type DenseToSparseSetOperationAttr func(optionalAttr)


// DenseToSparseSetOperationValidateIndices sets the optional validate_indices attribute to value.
// If not specified, defaults to true 
func DenseToSparseSetOperationValidateIndices(value bool) DenseToSparseSetOperationAttr {
	return func(m optionalAttr) {
		m["validate_indices"] = value
	}
}

// Applies set operation along last dimension of `Tensor` and `SparseTensor`.
//
// See SetOperationOp::SetOperationFromContext for values of `set_operation`.
// 
// Input `set2` is a `SparseTensor` represented by `set2_indices`, `set2_values`,
// and `set2_shape`. For `set2` ranked `n`, 1st `n-1` dimensions must be the same
// as `set1`. Dimension `n` contains values in a set, duplicates are allowed but
// ignored.
// 
// If `validate_indices` is `True`, this op validates the order and range of `set2`
// indices.
// 
// Output `result` is a `SparseTensor` represented by `result_indices`,
// `result_values`, and `result_shape`. For `set1` and `set2` ranked `n`, this
// has rank `n` and the same 1st `n-1` dimensions as `set1` and `set2`. The `nth`
// dimension contains the result of `set_operation` applied to the corresponding
// `[0...n-1]` dimension of `set`.
//
// Arguments:
//	set1: `Tensor` with rank `n`. 1st `n-1` dimensions must be the same as `set2`.
// Dimension `n` contains values in a set, duplicates are allowed but ignored.
//	set2_indices: 2D `Tensor`, indices of a `SparseTensor`. Must be in row-major
// order.
//	set2_values: 1D `Tensor`, values of a `SparseTensor`. Must be in row-major
// order.
//	set2_shape: 1D `Tensor`, shape of a `SparseTensor`. `set2_shape[0...n-1]` must
// be the same as the 1st `n-1` dimensions of `set1`, `result_shape[n]` is the
// max set size across `n-1` dimensions.
//	
//
// Returns 2D indices of a `SparseTensor`.1D values of a `SparseTensor`.1D `Tensor` shape of a `SparseTensor`. `result_shape[0...n-1]` is
// the same as the 1st `n-1` dimensions of `set1` and `set2`, `result_shape[n]`
// is the max result set size across all `0...n-1` dimensions.
func DenseToSparseSetOperation(scope *Scope, set1 tf.Output, set2_indices tf.Output, set2_values tf.Output, set2_shape tf.Output, set_operation string, optional ...DenseToSparseSetOperationAttr)(result_indices tf.Output, result_values tf.Output, result_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"set_operation": set_operation,}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "DenseToSparseSetOperation",
		Input: []tf.Input{
			set1, set2_indices, set2_values, set2_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// SparseTensorDenseMatMulAttr is an optional argument to SparseTensorDenseMatMul.
type SparseTensorDenseMatMulAttr func(optionalAttr)


// SparseTensorDenseMatMulAdjointA sets the optional adjoint_a attribute to value.
//
// value: Use the adjoint of A in the matrix multiply.  If A is complex, this
// is transpose(conj(A)).  Otherwise it's transpose(A).
// If not specified, defaults to false 
func SparseTensorDenseMatMulAdjointA(value bool) SparseTensorDenseMatMulAttr {
	return func(m optionalAttr) {
		m["adjoint_a"] = value
	}
}

// SparseTensorDenseMatMulAdjointB sets the optional adjoint_b attribute to value.
//
// value: Use the adjoint of B in the matrix multiply.  If B is complex, this
// is transpose(conj(B)).  Otherwise it's transpose(B).
// If not specified, defaults to false 
func SparseTensorDenseMatMulAdjointB(value bool) SparseTensorDenseMatMulAttr {
	return func(m optionalAttr) {
		m["adjoint_b"] = value
	}
}

// Multiply SparseTensor (of rank 2) "A" by dense matrix "B".
//
// No validity checking is performed on the indices of A.  However, the following
// input format is recommended for optimal behavior:
// 
// if adjoint_a == false:
//   A should be sorted in lexicographically increasing order.  Use SparseReorder
//   if you're not sure.
// if adjoint_a == true:
//   A should be sorted in order of increasing dimension 1 (i.e., "column major"
//   order instead of "row major" order).
//
// Arguments:
//	a_indices: 2-D.  The `indices` of the `SparseTensor`, size `[nnz, 2]` Matrix.
//	a_values: 1-D.  The `values` of the `SparseTensor`, size `[nnz]` Vector.
//	a_shape: 1-D.  The `shape` of the `SparseTensor`, size `[2]` Vector.
//	b: 2-D.  A dense Matrix.
func SparseTensorDenseMatMul(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b tf.Output, optional ...SparseTensorDenseMatMulAttr)(product tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseTensorDenseMatMul",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Deserialize and concatenate `SparseTensors` from a serialized minibatch.
//
// The input `serialized_sparse` must be a string matrix of shape `[N x 3]` where
// `N` is the minibatch size and the rows correspond to packed outputs of
// `SerializeSparse`.  The ranks of the original `SparseTensor` objects
// must all match.  When the final `SparseTensor` is created, it has rank one
// higher than the ranks of the incoming `SparseTensor` objects
// (they have been concatenated along a new row dimension).
// 
// The output `SparseTensor` object's shape values for all dimensions but the
// first are the max across the input `SparseTensor` objects' shape values
// for the corresponding dimensions.  Its first shape value is `N`, the minibatch
// size.
// 
// The input `SparseTensor` objects' indices are assumed ordered in
// standard lexicographic order.  If this is not the case, after this
// step run `SparseReorder` to restore index ordering.
// 
// For example, if the serialized input is a `[2 x 3]` matrix representing two
// original `SparseTensor` objects:
// 
//     index = [ 0]
//             [10]
//             [20]
//     values = [1, 2, 3]
//     shape = [50]
// 
// and
// 
//     index = [ 2]
//             [10]
//     values = [4, 5]
//     shape = [30]
// 
// then the final deserialized `SparseTensor` will be:
// 
//     index = [0  0]
//             [0 10]
//             [0 20]
//             [1  2]
//             [1 10]
//     values = [1, 2, 3, 4, 5]
//     shape = [2 50]
//
// Arguments:
//	serialized_sparse: 2-D, The `N` serialized `SparseTensor` objects.
// Must have 3 columns.
//	dtype: The `dtype` of the serialized `SparseTensor` objects.
func DeserializeManySparse(scope *Scope, serialized_sparse tf.Output, dtype tf.DataType)(sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"dtype": dtype,}
	opspec := tf.OpSpec{
		Type: "DeserializeManySparse",
		Input: []tf.Input{
			serialized_sparse, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// Serialize an `N`-minibatch `SparseTensor` into an `[N, 3]` string `Tensor`.
//
// The `SparseTensor` must have rank `R` greater than 1, and the first dimension
// is treated as the minibatch dimension.  Elements of the `SparseTensor`
// must be sorted in increasing order of this first dimension.  The serialized
// `SparseTensor` objects going into each row of `serialized_sparse` will have
// rank `R-1`.
// 
// The minibatch size `N` is extracted from `sparse_shape[0]`.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the minibatch `SparseTensor`.
//	sparse_values: 1-D.  The `values` of the minibatch `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the minibatch `SparseTensor`.
func SerializeManySparse(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output)(serialized_sparse tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SerializeManySparse",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Component-wise divides a SparseTensor by a dense Tensor.
//
// *Limitation*: this Op only broadcasts the dense side to the sparse side, but not
// the other direction.
//
// Arguments:
//	sp_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	sp_values: 1-D.  `N` non-empty values corresponding to `sp_indices`.
//	sp_shape: 1-D.  Shape of the input SparseTensor.
//	dense: `R`-D.  The dense Tensor operand.
//
// Returns 1-D.  The `N` values that are operated on.
func SparseDenseCwiseDiv(scope *Scope, sp_indices tf.Output, sp_values tf.Output, sp_shape tf.Output, dense tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseDenseCwiseDiv",
		Input: []tf.Input{
			sp_indices, sp_values, sp_shape, dense, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Serialize a `SparseTensor` into a string 3-vector (1-D `Tensor`) object.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the `SparseTensor`.
//	sparse_values: 1-D.  The `values` of the `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the `SparseTensor`.
func SerializeSparse(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output)(serialized_sparse tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SerializeSparse",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseToSparseSetOperationAttr is an optional argument to SparseToSparseSetOperation.
type SparseToSparseSetOperationAttr func(optionalAttr)


// SparseToSparseSetOperationValidateIndices sets the optional validate_indices attribute to value.
// If not specified, defaults to true 
func SparseToSparseSetOperationValidateIndices(value bool) SparseToSparseSetOperationAttr {
	return func(m optionalAttr) {
		m["validate_indices"] = value
	}
}

// Applies set operation along last dimension of 2 `SparseTensor` inputs.
//
// See SetOperationOp::SetOperationFromContext for values of `set_operation`.
// 
// If `validate_indices` is `True`, `SparseToSparseSetOperation` validates the
// order and range of `set1` and `set2` indices.
// 
// Input `set1` is a `SparseTensor` represented by `set1_indices`, `set1_values`,
// and `set1_shape`. For `set1` ranked `n`, 1st `n-1` dimensions must be the same
// as `set2`. Dimension `n` contains values in a set, duplicates are allowed but
// ignored.
// 
// Input `set2` is a `SparseTensor` represented by `set2_indices`, `set2_values`,
// and `set2_shape`. For `set2` ranked `n`, 1st `n-1` dimensions must be the same
// as `set1`. Dimension `n` contains values in a set, duplicates are allowed but
// ignored.
// 
// If `validate_indices` is `True`, this op validates the order and range of `set1`
// and `set2` indices.
// 
// Output `result` is a `SparseTensor` represented by `result_indices`,
// `result_values`, and `result_shape`. For `set1` and `set2` ranked `n`, this
// has rank `n` and the same 1st `n-1` dimensions as `set1` and `set2`. The `nth`
// dimension contains the result of `set_operation` applied to the corresponding
// `[0...n-1]` dimension of `set`.
//
// Arguments:
//	set1_indices: 2D `Tensor`, indices of a `SparseTensor`. Must be in row-major
// order.
//	set1_values: 1D `Tensor`, values of a `SparseTensor`. Must be in row-major
// order.
//	set1_shape: 1D `Tensor`, shape of a `SparseTensor`. `set1_shape[0...n-1]` must
// be the same as `set2_shape[0...n-1]`, `set1_shape[n]` is the
// max set size across `0...n-1` dimensions.
//	set2_indices: 2D `Tensor`, indices of a `SparseTensor`. Must be in row-major
// order.
//	set2_values: 1D `Tensor`, values of a `SparseTensor`. Must be in row-major
// order.
//	set2_shape: 1D `Tensor`, shape of a `SparseTensor`. `set2_shape[0...n-1]` must
// be the same as `set1_shape[0...n-1]`, `set2_shape[n]` is the
// max set size across `0...n-1` dimensions.
//	
//
// Returns 2D indices of a `SparseTensor`.1D values of a `SparseTensor`.1D `Tensor` shape of a `SparseTensor`. `result_shape[0...n-1]` is
// the same as the 1st `n-1` dimensions of `set1` and `set2`, `result_shape[n]`
// is the max result set size across all `0...n-1` dimensions.
func SparseToSparseSetOperation(scope *Scope, set1_indices tf.Output, set1_values tf.Output, set1_shape tf.Output, set2_indices tf.Output, set2_values tf.Output, set2_shape tf.Output, set_operation string, optional ...SparseToSparseSetOperationAttr)(result_indices tf.Output, result_values tf.Output, result_shape tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{"set_operation": set_operation,}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseToSparseSetOperation",
		Input: []tf.Input{
			set1_indices, set1_values, set1_shape, set2_indices, set2_values, set2_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1), op.Output(2)
}

// AddSparseToTensorsMapAttr is an optional argument to AddSparseToTensorsMap.
type AddSparseToTensorsMapAttr func(optionalAttr)


// AddSparseToTensorsMapContainer sets the optional container attribute to value.
//
// value: The container name for the `SparseTensorsMap` created by this op.
// If not specified, defaults to "" 
func AddSparseToTensorsMapContainer(value string) AddSparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["container"] = value
	}
}

// AddSparseToTensorsMapSharedName sets the optional shared_name attribute to value.
//
// value: The shared name for the `SparseTensorsMap` created by this op.
// If blank, the new Operation's unique name is used.
// If not specified, defaults to "" 
func AddSparseToTensorsMapSharedName(value string) AddSparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["shared_name"] = value
	}
}

// Add a `SparseTensor` to a `SparseTensorsMap` return its handle.
//
// A `SparseTensor` is represented by three tensors: `sparse_indices`,
// `sparse_values`, and `sparse_shape`.
// 
// This operator takes the given `SparseTensor` and adds it to a container
// object (a `SparseTensorsMap`).  A unique key within this container is generated
// in the form of an `int64`, and this is the value that is returned.
// 
// The `SparseTensor` can then be read out as part of a minibatch by passing
// the key as a vector element to `TakeManySparseFromTensorsMap`.  To ensure
// the correct `SparseTensorsMap` is accessed, ensure that the same
// `container` and `shared_name` are passed to that Op.  If no `shared_name`
// is provided here, instead use the *name* of the Operation created by calling
// `AddSparseToTensorsMap` as the `shared_name` passed to
// `TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the `SparseTensor`.
//	sparse_values: 1-D.  The `values` of the `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the `SparseTensor`.
//
// Returns 0-D.  The handle of the `SparseTensor` now stored in the
// `SparseTensorsMap`.
func AddSparseToTensorsMap(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output, optional ...AddSparseToTensorsMapAttr)(sparse_handle tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "AddSparseToTensorsMap",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes gradients for SparseSegmentMean.
//
// Returns tensor "output" with same shape as grad, except for dimension 0 whose
// value is output_dim0.
//
// Arguments:
//	grad: gradient propagated to the SparseSegmentMean op.
//	indices: indices passed to the corresponding SparseSegmentMean op.
//	segment_ids: segment_ids passed to the corresponding SparseSegmentMean op.
//	output_dim0: dimension 0 of "data" passed to SparseSegmentMean op.
func SparseSegmentMeanGrad(scope *Scope, grad tf.Output, indices tf.Output, segment_ids tf.Output, output_dim0 tf.Output)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSegmentMeanGrad",
		Input: []tf.Input{
			grad, indices, segment_ids, output_dim0, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// AddManySparseToTensorsMapAttr is an optional argument to AddManySparseToTensorsMap.
type AddManySparseToTensorsMapAttr func(optionalAttr)


// AddManySparseToTensorsMapContainer sets the optional container attribute to value.
//
// value: The container name for the `SparseTensorsMap` created by this op.
// If not specified, defaults to "" 
func AddManySparseToTensorsMapContainer(value string) AddManySparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["container"] = value
	}
}

// AddManySparseToTensorsMapSharedName sets the optional shared_name attribute to value.
//
// value: The shared name for the `SparseTensorsMap` created by this op.
// If blank, the new Operation's unique name is used.
// If not specified, defaults to "" 
func AddManySparseToTensorsMapSharedName(value string) AddManySparseToTensorsMapAttr {
	return func(m optionalAttr) {
		m["shared_name"] = value
	}
}

// Add an `N`-minibatch `SparseTensor` to a `SparseTensorsMap`, return `N` handles.
//
// A `SparseTensor` of rank `R` is represented by three tensors: `sparse_indices`,
// `sparse_values`, and `sparse_shape`, where
// 
// ```sparse_indices.shape[1] == sparse_shape.shape[0] == R```
// 
// An `N`-minibatch of `SparseTensor` objects is represented as a `SparseTensor`
// having a first `sparse_indices` column taking values between `[0, N)`, where
// the minibatch size `N == sparse_shape[0]`.
// 
// The input `SparseTensor` must have rank `R` greater than 1, and the first
// dimension is treated as the minibatch dimension.  Elements of the `SparseTensor`
// must be sorted in increasing order of this first dimension.  The stored
// `SparseTensor` objects pointed to by each row of the output `sparse_handles`
// will have rank `R-1`.
// 
// The `SparseTensor` values can then be read out as part of a minibatch by passing
// the given keys as vector elements to `TakeManySparseFromTensorsMap`.  To ensure
// the correct `SparseTensorsMap` is accessed, ensure that the same
// `container` and `shared_name` are passed to that Op.  If no `shared_name`
// is provided here, instead use the *name* of the Operation created by calling
// `AddManySparseToTensorsMap` as the `shared_name` passed to
// `TakeManySparseFromTensorsMap`.  Ensure the Operations are colocated.
//
// Arguments:
//	sparse_indices: 2-D.  The `indices` of the minibatch `SparseTensor`.
// `sparse_indices[:, 0]` must be ordered values in `[0, N)`.
//	sparse_values: 1-D.  The `values` of the minibatch `SparseTensor`.
//	sparse_shape: 1-D.  The `shape` of the minibatch `SparseTensor`.
// The minibatch size `N == sparse_shape[0]`.
//
// Returns 1-D.  The handles of the `SparseTensor` now stored in the
// `SparseTensorsMap`.  Shape: `[N]`.
func AddManySparseToTensorsMap(scope *Scope, sparse_indices tf.Output, sparse_values tf.Output, sparse_shape tf.Output, optional ...AddManySparseToTensorsMapAttr)(sparse_handles tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "AddManySparseToTensorsMap",
		Input: []tf.Input{
			sparse_indices, sparse_values, sparse_shape, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// SparseReduceSumAttr is an optional argument to SparseReduceSum.
type SparseReduceSumAttr func(optionalAttr)


// SparseReduceSumKeepDims sets the optional keep_dims attribute to value.
//
// value: If true, retain reduced dimensions with length 1.
// If not specified, defaults to false 
func SparseReduceSumKeepDims(value bool) SparseReduceSumAttr {
	return func(m optionalAttr) {
		m["keep_dims"] = value
	}
}

// Computes the sum of elements across dimensions of a SparseTensor.
//
// This Op takes a SparseTensor and is the sparse counterpart to
// `tf.reduce_sum()`.  In particular, this Op also returns a dense `Tensor`
// instead of a sparse one.
// 
// Reduces `sp_input` along the dimensions given in `reduction_axes`.  Unless
// `keep_dims` is true, the rank of the tensor is reduced by 1 for each entry in
// `reduction_axes`. If `keep_dims` is true, the reduced dimensions are retained
// with length 1.
// 
// If `reduction_axes` has no entries, all dimensions are reduced, and a tensor
// with a single element is returned.  Additionally, the axes can be negative,
// which are interpreted according to the indexing rules in Python.
//
// Arguments:
//	input_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, possibly not in canonical ordering.
//	input_values: 1-D.  `N` non-empty values corresponding to `input_indices`.
//	input_shape: 1-D.  Shape of the input SparseTensor.
//	reduction_axes: 1-D.  Length-`K` vector containing the reduction axes.
//
// Returns `R-K`-D.  The reduced Tensor.
func SparseReduceSum(scope *Scope, input_indices tf.Output, input_values tf.Output, input_shape tf.Output, reduction_axes tf.Output, optional ...SparseReduceSumAttr)(output tf.Output) {
	if scope.Err() != nil {
		return
	}
	attrs := map[string]interface{}{}
	for _, a := range optional {
		a(attrs)
	}
	opspec := tf.OpSpec{
		Type: "SparseReduceSum",
		Input: []tf.Input{
			input_indices, input_values, input_shape, reduction_axes, 
		},
		Attrs: attrs,
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Returns the element-wise max of two SparseTensors.
//
// Assumes the two SparseTensors have the same shape, i.e., no broadcasting.
//
// Arguments:
//	a_indices: 2-D.  `N x R` matrix with the indices of non-empty values in a
// SparseTensor, in the canonical lexicographic ordering.
//	a_values: 1-D.  `N` non-empty values corresponding to `a_indices`.
//	a_shape: 1-D.  Shape of the input SparseTensor.
//	b_indices: counterpart to `a_indices` for the other operand.
//	b_values: counterpart to `a_values` for the other operand; must be of the same dtype.
//	b_shape: counterpart to `a_shape` for the other operand; the two shapes must be equal.
//
// Returns 2-D.  The indices of the output SparseTensor.1-D.  The values of the output SparseTensor.
func SparseSparseMaximum(scope *Scope, a_indices tf.Output, a_values tf.Output, a_shape tf.Output, b_indices tf.Output, b_values tf.Output, b_shape tf.Output)(output_indices tf.Output, output_values tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseSparseMaximum",
		Input: []tf.Input{
			a_indices, a_values, a_shape, b_indices, b_values, b_shape, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}

// The gradient operator for the SparseAdd op.
//
// The SparseAdd op calculates A + B, where A, B, and the sum are all represented
// as `SparseTensor` objects.  This op takes in the upstream gradient w.r.t.
// non-empty values of the sum, and outputs the gradients w.r.t. the non-empty
// values of A and B.
//
// Arguments:
//	backprop_val_grad: 1-D with shape `[nnz(sum)]`.  The gradient with respect to
// the non-empty values of the sum.
//	a_indices: 2-D.  The `indices` of the `SparseTensor` A, size `[nnz(A), ndims]`.
//	b_indices: 2-D.  The `indices` of the `SparseTensor` B, size `[nnz(B), ndims]`.
//	sum_indices: 2-D.  The `indices` of the sum `SparseTensor`, size
// `[nnz(sum), ndims]`.
//
// Returns 1-D with shape `[nnz(A)]`. The gradient with respect to the
// non-empty values of A.1-D with shape `[nnz(B)]`. The gradient with respect to the
// non-empty values of B.
func SparseAddGrad(scope *Scope, backprop_val_grad tf.Output, a_indices tf.Output, b_indices tf.Output, sum_indices tf.Output)(a_val_grad tf.Output, b_val_grad tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "SparseAddGrad",
		Input: []tf.Input{
			backprop_val_grad, a_indices, b_indices, sum_indices, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0), op.Output(1)
}
