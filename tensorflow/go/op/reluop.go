// Copyright 2017 The TensorFlow Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// DO NOT EDIT
// This file was machine generated by github.com/ctava/tensorflow/tensorflow/go/genop/wrap
//
// WARNING: This generation of wrapper function for TensorFlow ops is in an
// experimental state. The generated API can change without notice.

package op

import tf "github.com/tensorflow/tensorflow/tensorflow/go"

// Computes rectified linear 6: `min(max(features, 0), 6)`.
func Relu6(scope *Scope, features tf.Output)(activations tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Relu6",
		Input: []tf.Input{
			features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear gradients for a Relu operation.
//
// Arguments:
//	gradients: The backpropagated gradients to the corresponding Relu operation.
//	features: The features passed as input to the corresponding Relu operation, OR
// the outputs of that operation (both work equivalently).
//
// Returns `gradients * (features > 0)`.
func ReluGrad(scope *Scope, gradients tf.Output, features tf.Output)(backprops tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "ReluGrad",
		Input: []tf.Input{
			gradients, features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear 6 gradients for a Relu6 operation.
//
// Arguments:
//	gradients: The backpropagated gradients to the corresponding Relu6 operation.
//	features: The features passed as input to the corresponding Relu6 operation.
//
// Returns The gradients:
// `gradients * (features > 0) * (features < 6)`.
func Relu6Grad(scope *Scope, gradients tf.Output, features tf.Output)(backprops tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Relu6Grad",
		Input: []tf.Input{
			gradients, features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear: `max(features, 0)`.
func Relu(scope *Scope, features tf.Output)(activations tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Relu",
		Input: []tf.Input{
			features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear 6: `min(max(features, 0), 6)`.
func Relu6(scope *Scope, features tf.Output)(activations tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Relu6",
		Input: []tf.Input{
			features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear gradients for a Relu operation.
//
// Arguments:
//	gradients: The backpropagated gradients to the corresponding Relu operation.
//	features: The features passed as input to the corresponding Relu operation, OR
// the outputs of that operation (both work equivalently).
//
// Returns `gradients * (features > 0)`.
func ReluGrad(scope *Scope, gradients tf.Output, features tf.Output)(backprops tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "ReluGrad",
		Input: []tf.Input{
			gradients, features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear 6 gradients for a Relu6 operation.
//
// Arguments:
//	gradients: The backpropagated gradients to the corresponding Relu6 operation.
//	features: The features passed as input to the corresponding Relu6 operation.
//
// Returns The gradients:
// `gradients * (features > 0) * (features < 6)`.
func Relu6Grad(scope *Scope, gradients tf.Output, features tf.Output)(backprops tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Relu6Grad",
		Input: []tf.Input{
			gradients, features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}

// Computes rectified linear: `max(features, 0)`.
func Relu(scope *Scope, features tf.Output)(activations tf.Output) {
	if scope.Err() != nil {
		return
	}
	opspec := tf.OpSpec{
		Type: "Relu",
		Input: []tf.Input{
			features, 
		},
	}
	op := scope.AddOperation(opspec)
	return op.Output(0)
}
